{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19acf93a-6d2c-4bd8-9f5a-4d8b274b34ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration simecek--Human_DNA_v0-3127ba11a87ac1a1\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/simecek___parquet/simecek--Human_DNA_v0-3127ba11a87ac1a1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3922e33c945430d9842e5e4ea192be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"simecek/Human_DNA_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c3879a0-3460-4337-887a-26f1e6b29451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmers_stride1(s, k=6):\n",
    "    return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
    "\n",
    "def batch_iterator(k=6, max_seqs=1_000):\n",
    "    for i in range(0, min(max_seqs, len(ds[\"train\"]))):\n",
    "        yield kmers_stride1(ds[\"train\"][i]['Seq'], k=k)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346177be-20da-4c49-8350-6ecba3d0b88e",
   "metadata": {},
   "source": [
    "## Training from scratch with `tokenizers` library\n",
    "\n",
    "I am able to train WordLevel tokenizer but it is slow and I still clearly miss some pieces (e.g. my tokenizer is not callable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab355fb-f28f-49ed-8174-1cf3e897af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4ccb82-fc28-48a4-b423-2bfce4d542e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Strip()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "#tokenizer.decoder = decoders.Whitespace()\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    min_frequency=3,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bbc58f7-c48c-4fb7-823a-6b065c4f0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 29.6 s, total: 1min 31s\n",
      "Wall time: 12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(ds[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d89eb4-0810-40f3-bca1-81b91563ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"ACCTGA CCCGGG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87549143-da57-4a60-beb2-62244cb0a89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCTGA', 'CCCGGG']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "622d542e-5718-4a5b-8ba2-ef8ca2842a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1641, 2872]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2db5357d-f0fb-4cf7-8a47-a47c75bbaf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1641"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"ACCTGA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da089e18-e366-4fa7-8319-028b2851298d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCCGGG', 'ACCTGA']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer.encode_batch([\"ACCTGA CCCGGG\", \"CCCGGG ACCTGA\"])\n",
    "output[1].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf773393-9f88-4e84-b3c0-54471df90fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1].attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab908c9-b331-49dc-a1dd-a94896522400",
   "metadata": {},
   "source": [
    "## Retraining old tokenizer\n",
    "\n",
    "This seems to work but on closer inspection it breaks 7-character word into several tokens. So it is not the best solution after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a6dde8-223a-4a47-b1b5-af1ff1dc30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer =  AutoTokenizer.from_pretrained(\"armheb/DNA_bert_6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aefb3d8c-347e-41c1-b37f-3a8f72dd3799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 47.2 s, sys: 4.8 s, total: 52 s\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "K = 7\n",
    "\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(batch_iterator(k=K, max_seqs=1000), vocab_size=4**K+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad44d96a-6d2a-4256-b7c1-cdc00bd8983b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 6349, 3110, 3361, 3197, 10016, 13645, 15506, 10920, 11, 21, 836, 105, 13083, 13447, 10313, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(s, k=6):\n",
    "    seq_split = \" \".join(kmers_stride1(s['Seq'], k))\n",
    "    return new_tokenizer(seq_split)\n",
    "\n",
    "tokenize_function({'Seq':'ACCTGCTGGACGATCATA'}, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46af2f17-d341-4178-9aef-d361e0404838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ACCTGCT CCTGCTG CTGCTGG TGCTGGA GCTGGAC CTGGACG TGGACGA GGACGAT GACGATC ACGATCA CGATCAT GATCATA [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.decode(tokenize_function({'Seq':'ACCTGCTGGACGATCATA'}, k=7)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed454d91-3116-4b22-8c21-0919379dfe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[UNK]', 1),\n",
       " ('[CLS]', 2),\n",
       " ('[SEP]', 3),\n",
       " ('[MASK]', 4),\n",
       " ('AAAAAA', 5),\n",
       " ('AAAAAT', 6),\n",
       " ('AAAAAC', 7),\n",
       " ('AAAAAG', 8),\n",
       " ('AAAATA', 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = list(old_tokenizer.vocab.items())\n",
    "sorted(dl, key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3ebde6-b982-40eb-ae24-165c2bb54e8f",
   "metadata": {},
   "source": [
    "## Contructing a tokenizer from the vocab file\n",
    "\n",
    "This would seems the most promising way but it is not working for me somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6591b5c8-1510-4630-9aa8-47296b92e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast, PreTrainedTokenizerFast, ElectraTokenizer, ElectraTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ccfdceee-36ef-435d-b3ad-c70b651c5aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384 16384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AAAAAAC', 'AAAAAAT', 'AAAAAAG', 'AAAAACA', 'AAAAACC', 'AAAAACT']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "K = 7\n",
    "\n",
    "alphabet = ('A', 'C', 'T', 'G')\n",
    "vocab = list(map(''.join, product(alphabet, repeat=K)))\n",
    "\n",
    "print(len(vocab), 4**K)\n",
    "vocab[1:7]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79b72236-798c-4f27-8337-022832eabad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vocab = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'] + vocab\n",
    "vocab_dict = {k: i for i, k in enumerate(full_vocab)}\n",
    "#vocab_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4facefb-5c5d-4cc8-8a78-4219b2daf09d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m old_tokenizer\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m vocab_dict\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "old_tokenizer.vocab = vocab_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa254dab-73ee-415b-8c1e-c160bde388e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for item in vocab:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d082c3f-de4f-4ad9-9ad3-84989e667d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='', vocab_size=16384, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertTokenizer(\"vocab.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d87aa4-0774-42ef-ae28-bf8205fd59ef",
   "metadata": {},
   "source": [
    "## Just add new words to the dictionary of the old tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd5acd78-577c-45b2-bba7-1cd807428b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.add_tokens(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eebfde3e-ef54-4ada-a4ac-378ee555e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5563, 9952, 11124, 15809, 18166, 11212, 16161, 19575, 16846, 5929, 11415, 16973, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(s, k=6):\n",
    "    seq_split = \" \".join(kmers_stride1(s['Seq'], k))\n",
    "    return old_tokenizer(seq_split)\n",
    "\n",
    "tokenize_function({'Seq':'ACCTGCTGGACGATCATA'}, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca1f62ec-bdee-4c48-be44-92a8c816b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] ACCTGCT CCTGCTG CTGCTGG TGCTGGA GCTGGAC CTGGACG TGGACGA GGACGAT GACGATC ACGATCA CGATCAT GATCATA [SEP]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_tokenizer.decode(tokenize_function({'Seq':'ACCTGCTGGACGATCATA'}, k=7)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73afb8cf-7287-43f2-8523-4ade36d4dba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4101"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but this is really suspicious\n",
    "old_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ef9ab-3f52-4a4c-a15d-d0ae021faeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myEnv]",
   "language": "python",
   "name": "conda-env-myEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
