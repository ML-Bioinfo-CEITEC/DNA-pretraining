{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gyck9b-I6tpY",
    "outputId": "eee2e692-7936-4250-9b27-789db827718d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('demo_coding_vs_intergenomic_seqs', 0), ('demo_human_or_worm', 0), ('human_enhancers_cohn', 0), ('human_enhancers_ensembl', 0), ('human_nontata_promoters', 0), ('human_ocr_ensembl', 0)]\n"
     ]
    }
   ],
   "source": [
    "### Parameters\n",
    "RANDOMIZE_WEIGHTS = False \n",
    "RESIZE_EMBEDDINGS = False #only used for using tokenizers with different vocab_size than orig. model\n",
    "\n",
    "OUTPUT_PATH = './DEBERTA_ALT_Sentencepiece30tokenizer_metrics.csv'\n",
    "\n",
    "MODEL_NAME = \"Vlasta/DNADebertaSentencepiece30k_continuation\"\n",
    "TOKENIZER_NAME = \"Vlasta/DNA_Sentencepiece_vocab_30000_max_tokenlen_100\"\n",
    "K = None\n",
    "STRIDE = None\n",
    "\n",
    "# All datasets\n",
    "# DATASETS = [('demo_coding_vs_intergenomic_seqs', 0),\n",
    "#  ('demo_human_or_worm', 0), ('human_enhancers_cohn', 0), ('human_enhancers_ensembl', 0),\n",
    "#  ('human_ensembl_regulatory', 0), ('human_nontata_promoters', 0), ('human_ocr_ensembl', 0)]\n",
    "\n",
    "# Quick check dataset\n",
    "# DATASETS = [('demo_human_or_worm', 0)]\n",
    "\n",
    "\n",
    "# Binary classification datasets (without human_ensembl_regulatory)\n",
    "DATASETS = [('demo_coding_vs_intergenomic_seqs', 0),\n",
    " ('demo_human_or_worm', 0), ('human_enhancers_cohn', 0), ('human_enhancers_ensembl', 0),\n",
    "  ('human_nontata_promoters', 0), ('human_ocr_ensembl', 0)]\n",
    "\n",
    "\n",
    "# if ensemble refuses connection - \"[Errno 104] Connection reset by peer\", use attribute use_cloud_cache=True\n",
    "BENCHMARKS_FOLDER = '/home/jovyan/.genomic_benchmarks'\n",
    "USE_CLOUD_CACHE = True\n",
    "# if less than 1, only this fraction of each dataset is used\n",
    "DATASET_THINING = 1 \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "ACCUMULATION = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 100 \n",
    "RUNS = 1\n",
    "\n",
    "print(DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "warmup_ratio = 0.05 #5 epochs (for 100 epochs total train)\n",
    "if(RANDOMIZE_WEIGHTS):\n",
    "    warmup_ratio = 0\n",
    "def get_trainargs():\n",
    "    return TrainingArguments(\n",
    "        'outputs', \n",
    "        learning_rate=LEARNING_RATE, \n",
    "        warmup_ratio=warmup_ratio, \n",
    "        lr_scheduler_type='linear',\n",
    "        fp16=True,\n",
    "        evaluation_strategy=\"epoch\", \n",
    "        per_device_train_batch_size=BATCH_SIZE, \n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=ACCUMULATION,\n",
    "        num_train_epochs=EPOCHS, \n",
    "        weight_decay=0.1, #ADJUSTED\n",
    "        save_strategy='epoch',\n",
    "        seed=randrange(1,10001), \n",
    "        report_to='none',\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "#early stopping 5 epochs\n",
    "callbacks= [\n",
    "    EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lq7oMV6baan7",
    "outputId": "6aa53001-2ddb-4cbf-ec5d-d296b168bf2c"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "if(K is not None and K>6):\n",
    "    alphabet = ('A', 'C', 'T', 'G')\n",
    "    vocab = list(map(''.join, product(alphabet, repeat=K)))\n",
    "    tokenizer.add_tokens(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "aolqScmmYaIs",
    "outputId": "5a706116-b020-47ba-a298-57e5d75080dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [2, 33, 1246, 2031, 6, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]ATGGAAAGAGGCACCATTCT[SEP]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kmers_strideK(s, k=K):\n",
    "    return [s[i:i + k] for i in range(0, len(s), k) if i + k <= len(s)]\n",
    "\n",
    "def kmers_stride1(s, k=K):\n",
    "    return [s[i:i + k] for i in range(0, len(s)-k+1)]\n",
    "\n",
    "if (STRIDE == 1):\n",
    "  kmers = kmers_stride1\n",
    "else:\n",
    "  kmers = kmers_strideK\n",
    "\n",
    "# function used for the actual tokenization\n",
    "if(K is not None):\n",
    "    def tok_func(x): return tokenizer(\" \".join(kmers(x[\"seq\"])), truncation=True)\n",
    "else:\n",
    "    def tok_func(x): return tokenizer(x[\"seq\"], truncation=True)\n",
    "\n",
    "# example\n",
    "example = tok_func({'seq': 'ATGGAAAGAGGCACCATTCT'})    \n",
    "print(example)\n",
    "tokenizer.decode(example['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfoAktdfUsl1"
   },
   "source": [
    "## Download benchmark datasets and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b1389c9d504e41b3bdc51a8805cfee5e",
      "cb813c47be474313bb9bdbec8327ddaf",
      "d33afcc4a33442d090d29ff1284e49ec",
      "dd67380f41b548cc8882b31219ab36de",
      "628aa13a3c8140be876e937c87d111be",
      "b59de534cb5b4d4486c6033c8f7bb4c2",
      "e3f54f15415a4a27bb77a7c06410db57",
      "01d30f8e4fde43fc8a3e4593d3932d47",
      "240183ed0048432e8f953293ea24bdce",
      "e9ca594da9214565bf101c528d74fc3d",
      "c04acfbcab1549e891f2320534af0400"
     ]
    },
    "id": "I3ZRsNqx7LNa",
    "outputId": "3562075e-a73f-4bc5-c977-f5d761762144"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ef58e8ac60433d9d1dd235275d6229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check.info import is_downloaded\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for dataset_name, dataset_version in tqdm(DATASETS):\n",
    "    if not is_downloaded(dataset_name):\n",
    "        download_dataset(dataset_name, version=dataset_version, use_cloud_cache=USE_CLOUD_CACHE)\n",
    "\n",
    "benchmark_root = Path(BENCHMARKS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to extract dataframe metrics row from training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_from_history(history, dataset_name):\n",
    "    eval_dicts = [x for x in history if 'eval_loss' in x]\n",
    "    test_dicts = [x for x in history if 'test_loss' in x]\n",
    "    test_log = test_dicts[0]\n",
    "    test_acc = test_log['test_accuracy']\n",
    "    test_f1 = test_log['test_f1']\n",
    "    test_loss = test_log['test_loss']\n",
    "    test_precision = test_log['test_precision']\n",
    "    test_recall = test_log['test_recall']\n",
    "    test_auroc_macro = test_log['test_rocauc_0_roc_auc']\n",
    "    test_auroc_weighted = test_log['test_rocauc_1_roc_auc']\n",
    "    test_pr_auc = test_log['test_pr_auc']\n",
    "    \n",
    "    \n",
    "    min_loss_dict = min(eval_dicts, key=lambda x: x['eval_loss'])\n",
    "    min_loss_epoch = min_loss_dict['epoch']\n",
    "    # max_f1_dict = max(eval_dicts, key=lambda x: x['eval_f1'])\n",
    "    # max_acc_dict = max(eval_dicts, key=lambda x: x['eval_accuracy'])\n",
    "    row = {\n",
    "        'dataset':dataset_name,\n",
    "        'test_acc':test_acc,\n",
    "        'test_f1':test_f1,\n",
    "        'test_loss':test_loss,\n",
    "        'test_precision':test_precision,\n",
    "        'test_recall':test_recall,\n",
    "        'test_auroc_macro':test_auroc_macro,\n",
    "        'test_auroc_weighted':test_auroc_weighted,\n",
    "        'test_pr_auc':test_pr_auc,\n",
    "        \n",
    "        'min_valid_loss_epoch':min_loss_epoch,\n",
    "        'min_valid_loss_log':min_loss_dict,\n",
    "        # 'max_valid_f1_log':max_f1_dict,\n",
    "        # 'max_valid_acc_log':max_acc_dict,\n",
    "    }\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cYuLrCnUz-K"
   },
   "source": [
    "## Looping through datasets, fine-tuning the model for each of them, logging metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "binary_metrics = evaluate.combine([\n",
    "    'accuracy',\n",
    "    'f1',\n",
    "    'recall',\n",
    "    'precision',\n",
    "    #Order of roc_auc matters for logging -> macro first, then weighted\n",
    "    evaluate.load('roc_auc', average='macro'),\n",
    "    evaluate.load('roc_auc', average='weighted'),\n",
    "    evaluate.load(\"Vlasta/pr_auc\"),\n",
    "])\n",
    "# binary_metrics.compute(references=[0,1,1,1], predictions=[0,0,1,1], prediction_scores=[0.4,0.3,0.6,0.9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_C7mdPSn-0Pl"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d838ccdfc14a48a0af7efa793dd09d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a89b43d79240f0afa408ab5d237d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e195d9a098064dc18921e80a5bd038a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b89cdfd6a04793b76f9e69452be393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 60000\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 93700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8433' max='93700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8433/93700 09:24 < 1:35:06, 14.94 it/s, Epoch 8/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.599100</td>\n",
       "      <td>0.283258</td>\n",
       "      <td>0.884867</td>\n",
       "      <td>0.882860</td>\n",
       "      <td>0.867849</td>\n",
       "      <td>0.898399</td>\n",
       "      <td>0.954499</td>\n",
       "      <td>0.954499</td>\n",
       "      <td>0.954641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279800</td>\n",
       "      <td>0.241445</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.901121</td>\n",
       "      <td>0.884118</td>\n",
       "      <td>0.918792</td>\n",
       "      <td>0.965532</td>\n",
       "      <td>0.965532</td>\n",
       "      <td>0.967061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.233200</td>\n",
       "      <td>0.245766</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.896828</td>\n",
       "      <td>0.850247</td>\n",
       "      <td>0.948810</td>\n",
       "      <td>0.970586</td>\n",
       "      <td>0.970586</td>\n",
       "      <td>0.971506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>0.224972</td>\n",
       "      <td>0.911733</td>\n",
       "      <td>0.912526</td>\n",
       "      <td>0.920923</td>\n",
       "      <td>0.904282</td>\n",
       "      <td>0.971371</td>\n",
       "      <td>0.971371</td>\n",
       "      <td>0.972409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.165200</td>\n",
       "      <td>0.227206</td>\n",
       "      <td>0.911867</td>\n",
       "      <td>0.911228</td>\n",
       "      <td>0.904787</td>\n",
       "      <td>0.917760</td>\n",
       "      <td>0.971247</td>\n",
       "      <td>0.971247</td>\n",
       "      <td>0.972377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.123700</td>\n",
       "      <td>0.281151</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>0.910486</td>\n",
       "      <td>0.917589</td>\n",
       "      <td>0.903493</td>\n",
       "      <td>0.968955</td>\n",
       "      <td>0.968955</td>\n",
       "      <td>0.970075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.332428</td>\n",
       "      <td>0.902933</td>\n",
       "      <td>0.905147</td>\n",
       "      <td>0.926390</td>\n",
       "      <td>0.884855</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.965079</td>\n",
       "      <td>0.966332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.381206</td>\n",
       "      <td>0.904667</td>\n",
       "      <td>0.905160</td>\n",
       "      <td>0.909988</td>\n",
       "      <td>0.900383</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.964359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.385259</td>\n",
       "      <td>0.905933</td>\n",
       "      <td>0.906240</td>\n",
       "      <td>0.909321</td>\n",
       "      <td>0.903179</td>\n",
       "      <td>0.962628</td>\n",
       "      <td>0.962628</td>\n",
       "      <td>0.962295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-937\n",
      "Configuration saved in outputs/checkpoint-937/config.json\n",
      "Model weights saved in outputs/checkpoint-937/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-937/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-937/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1874\n",
      "Configuration saved in outputs/checkpoint-1874/config.json\n",
      "Model weights saved in outputs/checkpoint-1874/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1874/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1874/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2811\n",
      "Configuration saved in outputs/checkpoint-2811/config.json\n",
      "Model weights saved in outputs/checkpoint-2811/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2811/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2811/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-3748\n",
      "Configuration saved in outputs/checkpoint-3748/config.json\n",
      "Model weights saved in outputs/checkpoint-3748/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3748/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3748/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-4685\n",
      "Configuration saved in outputs/checkpoint-4685/config.json\n",
      "Model weights saved in outputs/checkpoint-4685/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-4685/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-4685/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-5622\n",
      "Configuration saved in outputs/checkpoint-5622/config.json\n",
      "Model weights saved in outputs/checkpoint-5622/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-5622/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-5622/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-6559\n",
      "Configuration saved in outputs/checkpoint-6559/config.json\n",
      "Model weights saved in outputs/checkpoint-6559/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-6559/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-6559/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-7496\n",
      "Configuration saved in outputs/checkpoint-7496/config.json\n",
      "Model weights saved in outputs/checkpoint-7496/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-7496/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-7496/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-8433\n",
      "Configuration saved in outputs/checkpoint-8433/config.json\n",
      "Model weights saved in outputs/checkpoint-8433/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-8433/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-8433/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-3748 (score: 0.22497177124023438).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcab923433a745a79f36c0f684e790ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3718466027884ecf8f409ac89f346b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5d1dede9444bc5993ebe1c850998fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/ed63502d3bd126c04d68de5b513e238d890d803f275c3974150696225df86729.7607ffdb351882cfaefe9c35d56c9ae147178c0ea5705855ec06b5f18f11a116\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"Vlasta/DNADebertaSentencepiece30k_continuation\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": null,\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/76123980584a06476c3e53ec7adc1290ff75cddd27d3b12a86294641b81fb536.c29502a2d5e49ca82f33b30a15f5274d6835444ed8a136313ebbbb73ec2464b5\n",
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 60000\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 93700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7496' max='93700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7496/93700 20:03 < 3:50:43, 6.23 it/s, Epoch 7/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.613000</td>\n",
       "      <td>0.191994</td>\n",
       "      <td>0.936933</td>\n",
       "      <td>0.936722</td>\n",
       "      <td>0.933724</td>\n",
       "      <td>0.939740</td>\n",
       "      <td>0.982845</td>\n",
       "      <td>0.982845</td>\n",
       "      <td>0.984364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.175500</td>\n",
       "      <td>0.130279</td>\n",
       "      <td>0.952467</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.950793</td>\n",
       "      <td>0.953974</td>\n",
       "      <td>0.990841</td>\n",
       "      <td>0.990841</td>\n",
       "      <td>0.991315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.113749</td>\n",
       "      <td>0.958067</td>\n",
       "      <td>0.958325</td>\n",
       "      <td>0.964395</td>\n",
       "      <td>0.952331</td>\n",
       "      <td>0.992759</td>\n",
       "      <td>0.992759</td>\n",
       "      <td>0.992988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.114867</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>0.958488</td>\n",
       "      <td>0.946793</td>\n",
       "      <td>0.970476</td>\n",
       "      <td>0.992773</td>\n",
       "      <td>0.992773</td>\n",
       "      <td>0.993063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>0.118304</td>\n",
       "      <td>0.958667</td>\n",
       "      <td>0.958389</td>\n",
       "      <td>0.952127</td>\n",
       "      <td>0.964734</td>\n",
       "      <td>0.992516</td>\n",
       "      <td>0.992516</td>\n",
       "      <td>0.992782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053900</td>\n",
       "      <td>0.220319</td>\n",
       "      <td>0.943867</td>\n",
       "      <td>0.941455</td>\n",
       "      <td>0.902787</td>\n",
       "      <td>0.983583</td>\n",
       "      <td>0.992512</td>\n",
       "      <td>0.992512</td>\n",
       "      <td>0.992808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.201387</td>\n",
       "      <td>0.954333</td>\n",
       "      <td>0.953449</td>\n",
       "      <td>0.935458</td>\n",
       "      <td>0.972145</td>\n",
       "      <td>0.991913</td>\n",
       "      <td>0.991913</td>\n",
       "      <td>0.992166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.024100</td>\n",
       "      <td>0.232413</td>\n",
       "      <td>0.957067</td>\n",
       "      <td>0.957090</td>\n",
       "      <td>0.957728</td>\n",
       "      <td>0.956452</td>\n",
       "      <td>0.991680</td>\n",
       "      <td>0.991680</td>\n",
       "      <td>0.992039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-937\n",
      "Configuration saved in outputs/checkpoint-937/config.json\n",
      "Model weights saved in outputs/checkpoint-937/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-937/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-937/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1874\n",
      "Configuration saved in outputs/checkpoint-1874/config.json\n",
      "Model weights saved in outputs/checkpoint-1874/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1874/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1874/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2811\n",
      "Configuration saved in outputs/checkpoint-2811/config.json\n",
      "Model weights saved in outputs/checkpoint-2811/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2811/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2811/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-3748\n",
      "Configuration saved in outputs/checkpoint-3748/config.json\n",
      "Model weights saved in outputs/checkpoint-3748/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3748/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3748/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-4685\n",
      "Configuration saved in outputs/checkpoint-4685/config.json\n",
      "Model weights saved in outputs/checkpoint-4685/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-4685/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-4685/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-5622\n",
      "Configuration saved in outputs/checkpoint-5622/config.json\n",
      "Model weights saved in outputs/checkpoint-5622/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-5622/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-5622/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-6559\n",
      "Configuration saved in outputs/checkpoint-6559/config.json\n",
      "Model weights saved in outputs/checkpoint-6559/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-6559/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-6559/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-7496\n",
      "Configuration saved in outputs/checkpoint-7496/config.json\n",
      "Model weights saved in outputs/checkpoint-7496/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-7496/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-7496/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-2811 (score: 0.11374924331903458).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [782/782 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d3dc2bdbb44345a58031b463404fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27791 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35521f05394f46309baea0009a530e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fccf742c43a497fa124441e2ccbf445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/ed63502d3bd126c04d68de5b513e238d890d803f275c3974150696225df86729.7607ffdb351882cfaefe9c35d56c9ae147178c0ea5705855ec06b5f18f11a116\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"Vlasta/DNADebertaSentencepiece30k_continuation\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": null,\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/76123980584a06476c3e53ec7adc1290ff75cddd27d3b12a86294641b81fb536.c29502a2d5e49ca82f33b30a15f5274d6835444ed8a136313ebbbb73ec2464b5\n",
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 16674\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 26100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2349' max='26100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2349/26100 04:38 < 47:02, 8.41 it/s, Epoch 9/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.565126</td>\n",
       "      <td>0.710242</td>\n",
       "      <td>0.706654</td>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.721011</td>\n",
       "      <td>0.792244</td>\n",
       "      <td>0.792244</td>\n",
       "      <td>0.796434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.526436</td>\n",
       "      <td>0.732550</td>\n",
       "      <td>0.737709</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.728963</td>\n",
       "      <td>0.815443</td>\n",
       "      <td>0.815443</td>\n",
       "      <td>0.817646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.524400</td>\n",
       "      <td>0.729432</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.709664</td>\n",
       "      <td>0.818924</td>\n",
       "      <td>0.818924</td>\n",
       "      <td>0.822771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.520463</td>\n",
       "      <td>0.735668</td>\n",
       "      <td>0.750679</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.715086</td>\n",
       "      <td>0.824369</td>\n",
       "      <td>0.824369</td>\n",
       "      <td>0.827240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.541126</td>\n",
       "      <td>0.734948</td>\n",
       "      <td>0.738832</td>\n",
       "      <td>0.744286</td>\n",
       "      <td>0.733458</td>\n",
       "      <td>0.818943</td>\n",
       "      <td>0.818943</td>\n",
       "      <td>0.822237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.613047</td>\n",
       "      <td>0.721996</td>\n",
       "      <td>0.731028</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.712992</td>\n",
       "      <td>0.809517</td>\n",
       "      <td>0.809517</td>\n",
       "      <td>0.814759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.713371</td>\n",
       "      <td>0.714080</td>\n",
       "      <td>0.712633</td>\n",
       "      <td>0.703810</td>\n",
       "      <td>0.721680</td>\n",
       "      <td>0.791680</td>\n",
       "      <td>0.791680</td>\n",
       "      <td>0.798320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.710002</td>\n",
       "      <td>0.719944</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.700947</td>\n",
       "      <td>0.787132</td>\n",
       "      <td>0.787132</td>\n",
       "      <td>0.785856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.268600</td>\n",
       "      <td>0.966406</td>\n",
       "      <td>0.695131</td>\n",
       "      <td>0.723275</td>\n",
       "      <td>0.790952</td>\n",
       "      <td>0.666266</td>\n",
       "      <td>0.773741</td>\n",
       "      <td>0.773741</td>\n",
       "      <td>0.755454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-261\n",
      "Configuration saved in outputs/checkpoint-261/config.json\n",
      "Model weights saved in outputs/checkpoint-261/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-261/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-261/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-522\n",
      "Configuration saved in outputs/checkpoint-522/config.json\n",
      "Model weights saved in outputs/checkpoint-522/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-522/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-522/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-783\n",
      "Configuration saved in outputs/checkpoint-783/config.json\n",
      "Model weights saved in outputs/checkpoint-783/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-783/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-783/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1044\n",
      "Configuration saved in outputs/checkpoint-1044/config.json\n",
      "Model weights saved in outputs/checkpoint-1044/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1044/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1044/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1305\n",
      "Configuration saved in outputs/checkpoint-1305/config.json\n",
      "Model weights saved in outputs/checkpoint-1305/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1305/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1305/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1566\n",
      "Configuration saved in outputs/checkpoint-1566/config.json\n",
      "Model weights saved in outputs/checkpoint-1566/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1566/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1566/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1827\n",
      "Configuration saved in outputs/checkpoint-1827/config.json\n",
      "Model weights saved in outputs/checkpoint-1827/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1827/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1827/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2088\n",
      "Configuration saved in outputs/checkpoint-2088/config.json\n",
      "Model weights saved in outputs/checkpoint-2088/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2088/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2088/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4169\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2349\n",
      "Configuration saved in outputs/checkpoint-2349/config.json\n",
      "Model weights saved in outputs/checkpoint-2349/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2349/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2349/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-1044 (score: 0.520462691783905).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6948\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='218' max='218' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [218/218 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a4825b1d284a5ab3d337219e9db767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123872 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14604901b94a4e979ca6a422b7e9a08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252fd621366b42dc8a6baa905d3af410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/124 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/ed63502d3bd126c04d68de5b513e238d890d803f275c3974150696225df86729.7607ffdb351882cfaefe9c35d56c9ae147178c0ea5705855ec06b5f18f11a116\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"Vlasta/DNADebertaSentencepiece30k_continuation\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": null,\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/76123980584a06476c3e53ec7adc1290ff75cddd27d3b12a86294641b81fb536.c29502a2d5e49ca82f33b30a15f5274d6835444ed8a136313ebbbb73ec2464b5\n",
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 74321\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 116100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11610' max='116100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11610/116100 21:29 < 3:13:26, 9.00 it/s, Epoch 9/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.558600</td>\n",
       "      <td>0.469307</td>\n",
       "      <td>0.778699</td>\n",
       "      <td>0.789431</td>\n",
       "      <td>0.824738</td>\n",
       "      <td>0.757022</td>\n",
       "      <td>0.860826</td>\n",
       "      <td>0.860826</td>\n",
       "      <td>0.848010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.436100</td>\n",
       "      <td>0.400393</td>\n",
       "      <td>0.814380</td>\n",
       "      <td>0.820580</td>\n",
       "      <td>0.843890</td>\n",
       "      <td>0.798522</td>\n",
       "      <td>0.901147</td>\n",
       "      <td>0.901147</td>\n",
       "      <td>0.897061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.383046</td>\n",
       "      <td>0.824875</td>\n",
       "      <td>0.822030</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.840792</td>\n",
       "      <td>0.911419</td>\n",
       "      <td>0.911419</td>\n",
       "      <td>0.908795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.348400</td>\n",
       "      <td>0.368944</td>\n",
       "      <td>0.838329</td>\n",
       "      <td>0.841761</td>\n",
       "      <td>0.854911</td>\n",
       "      <td>0.829010</td>\n",
       "      <td>0.919512</td>\n",
       "      <td>0.919512</td>\n",
       "      <td>0.917203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.361806</td>\n",
       "      <td>0.846940</td>\n",
       "      <td>0.848820</td>\n",
       "      <td>0.854269</td>\n",
       "      <td>0.843440</td>\n",
       "      <td>0.923815</td>\n",
       "      <td>0.923815</td>\n",
       "      <td>0.920962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.390114</td>\n",
       "      <td>0.849201</td>\n",
       "      <td>0.852464</td>\n",
       "      <td>0.866146</td>\n",
       "      <td>0.839208</td>\n",
       "      <td>0.924273</td>\n",
       "      <td>0.924273</td>\n",
       "      <td>0.921725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.397537</td>\n",
       "      <td>0.856466</td>\n",
       "      <td>0.859209</td>\n",
       "      <td>0.870747</td>\n",
       "      <td>0.847973</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.924168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.495871</td>\n",
       "      <td>0.855228</td>\n",
       "      <td>0.858540</td>\n",
       "      <td>0.873422</td>\n",
       "      <td>0.844157</td>\n",
       "      <td>0.925885</td>\n",
       "      <td>0.925885</td>\n",
       "      <td>0.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.579684</td>\n",
       "      <td>0.846940</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.893537</td>\n",
       "      <td>0.818725</td>\n",
       "      <td>0.923574</td>\n",
       "      <td>0.923574</td>\n",
       "      <td>0.917592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078900</td>\n",
       "      <td>0.637086</td>\n",
       "      <td>0.847479</td>\n",
       "      <td>0.855423</td>\n",
       "      <td>0.897068</td>\n",
       "      <td>0.817473</td>\n",
       "      <td>0.922630</td>\n",
       "      <td>0.922630</td>\n",
       "      <td>0.913320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1161\n",
      "Configuration saved in outputs/checkpoint-1161/config.json\n",
      "Model weights saved in outputs/checkpoint-1161/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1161/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1161/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2322\n",
      "Configuration saved in outputs/checkpoint-2322/config.json\n",
      "Model weights saved in outputs/checkpoint-2322/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2322/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2322/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-3483\n",
      "Configuration saved in outputs/checkpoint-3483/config.json\n",
      "Model weights saved in outputs/checkpoint-3483/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3483/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3483/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-4644\n",
      "Configuration saved in outputs/checkpoint-4644/config.json\n",
      "Model weights saved in outputs/checkpoint-4644/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-4644/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-4644/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-5805\n",
      "Configuration saved in outputs/checkpoint-5805/config.json\n",
      "Model weights saved in outputs/checkpoint-5805/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-5805/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-5805/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-6966\n",
      "Configuration saved in outputs/checkpoint-6966/config.json\n",
      "Model weights saved in outputs/checkpoint-6966/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-6966/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-6966/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-8127\n",
      "Configuration saved in outputs/checkpoint-8127/config.json\n",
      "Model weights saved in outputs/checkpoint-8127/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-8127/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-8127/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-9288\n",
      "Configuration saved in outputs/checkpoint-9288/config.json\n",
      "Model weights saved in outputs/checkpoint-9288/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-9288/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-9288/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-10449\n",
      "Configuration saved in outputs/checkpoint-10449/config.json\n",
      "Model weights saved in outputs/checkpoint-10449/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-10449/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-10449/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 18581\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-11610\n",
      "Configuration saved in outputs/checkpoint-11610/config.json\n",
      "Model weights saved in outputs/checkpoint-11610/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-11610/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-11610/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-5805 (score: 0.36180582642555237).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30970\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='968' max='968' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [968/968 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3769fb5f0984b55b77b29f4e8325af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25284 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c5e74fe69845879df267819a24ad50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68befdcc629b4f9fa5e370dd7e5a9b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/ed63502d3bd126c04d68de5b513e238d890d803f275c3974150696225df86729.7607ffdb351882cfaefe9c35d56c9ae147178c0ea5705855ec06b5f18f11a116\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"Vlasta/DNADebertaSentencepiece30k_continuation\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": null,\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/76123980584a06476c3e53ec7adc1290ff75cddd27d3b12a86294641b81fb536.c29502a2d5e49ca82f33b30a15f5274d6835444ed8a136313ebbbb73ec2464b5\n",
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 13000\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 20300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2436' max='20300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2436/20300 03:13 < 23:37, 12.60 it/s, Epoch 11/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.381016</td>\n",
       "      <td>0.876615</td>\n",
       "      <td>0.674249</td>\n",
       "      <td>0.557796</td>\n",
       "      <td>0.852156</td>\n",
       "      <td>0.870880</td>\n",
       "      <td>0.870880</td>\n",
       "      <td>0.775302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285971</td>\n",
       "      <td>0.901231</td>\n",
       "      <td>0.757002</td>\n",
       "      <td>0.672043</td>\n",
       "      <td>0.866551</td>\n",
       "      <td>0.903780</td>\n",
       "      <td>0.903780</td>\n",
       "      <td>0.830501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.268226</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.866591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.438500</td>\n",
       "      <td>0.219524</td>\n",
       "      <td>0.928923</td>\n",
       "      <td>0.823259</td>\n",
       "      <td>0.723118</td>\n",
       "      <td>0.955595</td>\n",
       "      <td>0.933403</td>\n",
       "      <td>0.933403</td>\n",
       "      <td>0.893882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.216545</td>\n",
       "      <td>0.936615</td>\n",
       "      <td>0.843465</td>\n",
       "      <td>0.745968</td>\n",
       "      <td>0.970280</td>\n",
       "      <td>0.945450</td>\n",
       "      <td>0.945450</td>\n",
       "      <td>0.911725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.215798</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.852161</td>\n",
       "      <td>0.755376</td>\n",
       "      <td>0.977391</td>\n",
       "      <td>0.950534</td>\n",
       "      <td>0.950534</td>\n",
       "      <td>0.924105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.234400</td>\n",
       "      <td>0.190081</td>\n",
       "      <td>0.950462</td>\n",
       "      <td>0.885735</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>0.938346</td>\n",
       "      <td>0.956733</td>\n",
       "      <td>0.956733</td>\n",
       "      <td>0.929779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.230430</td>\n",
       "      <td>0.945846</td>\n",
       "      <td>0.869822</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.967105</td>\n",
       "      <td>0.946946</td>\n",
       "      <td>0.946946</td>\n",
       "      <td>0.911958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.227025</td>\n",
       "      <td>0.948923</td>\n",
       "      <td>0.879884</td>\n",
       "      <td>0.817204</td>\n",
       "      <td>0.952978</td>\n",
       "      <td>0.955002</td>\n",
       "      <td>0.955002</td>\n",
       "      <td>0.918219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.273164</td>\n",
       "      <td>0.947077</td>\n",
       "      <td>0.872214</td>\n",
       "      <td>0.788978</td>\n",
       "      <td>0.975083</td>\n",
       "      <td>0.959392</td>\n",
       "      <td>0.959392</td>\n",
       "      <td>0.927102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.318716</td>\n",
       "      <td>0.943692</td>\n",
       "      <td>0.861469</td>\n",
       "      <td>0.764785</td>\n",
       "      <td>0.986135</td>\n",
       "      <td>0.941658</td>\n",
       "      <td>0.941658</td>\n",
       "      <td>0.909725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.053500</td>\n",
       "      <td>0.349743</td>\n",
       "      <td>0.943385</td>\n",
       "      <td>0.860817</td>\n",
       "      <td>0.764785</td>\n",
       "      <td>0.984429</td>\n",
       "      <td>0.950381</td>\n",
       "      <td>0.950381</td>\n",
       "      <td>0.925247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-203\n",
      "Configuration saved in outputs/checkpoint-203/config.json\n",
      "Model weights saved in outputs/checkpoint-203/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-203/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-203/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-406\n",
      "Configuration saved in outputs/checkpoint-406/config.json\n",
      "Model weights saved in outputs/checkpoint-406/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-406/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-406/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-609\n",
      "Configuration saved in outputs/checkpoint-609/config.json\n",
      "Model weights saved in outputs/checkpoint-609/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-609/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-609/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-812\n",
      "Configuration saved in outputs/checkpoint-812/config.json\n",
      "Model weights saved in outputs/checkpoint-812/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-812/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-812/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1015\n",
      "Configuration saved in outputs/checkpoint-1015/config.json\n",
      "Model weights saved in outputs/checkpoint-1015/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1015/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1015/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1218\n",
      "Configuration saved in outputs/checkpoint-1218/config.json\n",
      "Model weights saved in outputs/checkpoint-1218/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1218/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1218/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1421\n",
      "Configuration saved in outputs/checkpoint-1421/config.json\n",
      "Model weights saved in outputs/checkpoint-1421/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1421/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1421/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1624\n",
      "Configuration saved in outputs/checkpoint-1624/config.json\n",
      "Model weights saved in outputs/checkpoint-1624/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1624/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1624/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1827\n",
      "Configuration saved in outputs/checkpoint-1827/config.json\n",
      "Model weights saved in outputs/checkpoint-1827/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1827/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1827/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2030\n",
      "Configuration saved in outputs/checkpoint-2030/config.json\n",
      "Model weights saved in outputs/checkpoint-2030/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2030/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2030/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2233\n",
      "Configuration saved in outputs/checkpoint-2233/config.json\n",
      "Model weights saved in outputs/checkpoint-2233/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2233/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2233/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2436\n",
      "Configuration saved in outputs/checkpoint-2436/config.json\n",
      "Model weights saved in outputs/checkpoint-2436/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2436/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2436/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-1421 (score: 0.19008123874664307).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9034\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='283' max='283' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [283/283 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e690f4474e541a5aa110fe6d2db4260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/139804 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee34112bee96448da01127c64388ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8035df9116441039b2291b4e5120aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/config.json from cache at /home/jovyan/.cache/huggingface/transformers/ed63502d3bd126c04d68de5b513e238d890d803f275c3974150696225df86729.7607ffdb351882cfaefe9c35d56c9ae147178c0ea5705855ec06b5f18f11a116\n",
      "Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"Vlasta/DNADebertaSentencepiece30k_continuation\",\n",
      "  \"architectures\": [\n",
      "    \"DebertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": null,\n",
      "  \"position_biased_input\": true,\n",
      "  \"relative_attention\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Vlasta/DNADebertaSentencepiece30k_continuation/resolve/main/pytorch_model.bin from cache at /home/jovyan/.cache/huggingface/transformers/76123980584a06476c3e53ec7adc1290ff75cddd27d3b12a86294641b81fb536.c29502a2d5e49ca82f33b30a15f5274d6835444ed8a136313ebbbb73ec2464b5\n",
      "Some weights of the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation were not used when initializing DebertaForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at Vlasta/DNADebertaSentencepiece30k_continuation and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "Using amp half precision backend\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 83881\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 131100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13110' max='131100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 13110/131100 25:34 < 3:50:12, 8.54 it/s, Epoch 10/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Rocauc 0 Roc Auc</th>\n",
       "      <th>Rocauc 1 Roc Auc</th>\n",
       "      <th>Pr Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.659600</td>\n",
       "      <td>0.614597</td>\n",
       "      <td>0.664680</td>\n",
       "      <td>0.648470</td>\n",
       "      <td>0.611310</td>\n",
       "      <td>0.690441</td>\n",
       "      <td>0.731375</td>\n",
       "      <td>0.731375</td>\n",
       "      <td>0.709439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.572800</td>\n",
       "      <td>0.561747</td>\n",
       "      <td>0.703543</td>\n",
       "      <td>0.696124</td>\n",
       "      <td>0.671159</td>\n",
       "      <td>0.723018</td>\n",
       "      <td>0.784298</td>\n",
       "      <td>0.784298</td>\n",
       "      <td>0.771986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.558952</td>\n",
       "      <td>0.711459</td>\n",
       "      <td>0.683343</td>\n",
       "      <td>0.615363</td>\n",
       "      <td>0.768208</td>\n",
       "      <td>0.805924</td>\n",
       "      <td>0.805924</td>\n",
       "      <td>0.796432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.495600</td>\n",
       "      <td>0.526706</td>\n",
       "      <td>0.737399</td>\n",
       "      <td>0.738497</td>\n",
       "      <td>0.732893</td>\n",
       "      <td>0.744186</td>\n",
       "      <td>0.816603</td>\n",
       "      <td>0.816603</td>\n",
       "      <td>0.803936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.525087</td>\n",
       "      <td>0.739307</td>\n",
       "      <td>0.756275</td>\n",
       "      <td>0.799434</td>\n",
       "      <td>0.717537</td>\n",
       "      <td>0.818205</td>\n",
       "      <td>0.818205</td>\n",
       "      <td>0.808397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>0.576108</td>\n",
       "      <td>0.732345</td>\n",
       "      <td>0.734998</td>\n",
       "      <td>0.733648</td>\n",
       "      <td>0.736354</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.811497</td>\n",
       "      <td>0.796845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.635213</td>\n",
       "      <td>0.727862</td>\n",
       "      <td>0.729744</td>\n",
       "      <td>0.726202</td>\n",
       "      <td>0.733321</td>\n",
       "      <td>0.806792</td>\n",
       "      <td>0.806792</td>\n",
       "      <td>0.792848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.248400</td>\n",
       "      <td>0.792949</td>\n",
       "      <td>0.712937</td>\n",
       "      <td>0.696328</td>\n",
       "      <td>0.650518</td>\n",
       "      <td>0.749077</td>\n",
       "      <td>0.799116</td>\n",
       "      <td>0.799116</td>\n",
       "      <td>0.782276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.201400</td>\n",
       "      <td>0.826145</td>\n",
       "      <td>0.712269</td>\n",
       "      <td>0.706546</td>\n",
       "      <td>0.684637</td>\n",
       "      <td>0.729904</td>\n",
       "      <td>0.791114</td>\n",
       "      <td>0.791114</td>\n",
       "      <td>0.775105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.952245</td>\n",
       "      <td>0.713748</td>\n",
       "      <td>0.717466</td>\n",
       "      <td>0.718379</td>\n",
       "      <td>0.716555</td>\n",
       "      <td>0.786995</td>\n",
       "      <td>0.786995</td>\n",
       "      <td>0.763469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-1311\n",
      "Configuration saved in outputs/checkpoint-1311/config.json\n",
      "Model weights saved in outputs/checkpoint-1311/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-1311/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-1311/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-2622\n",
      "Configuration saved in outputs/checkpoint-2622/config.json\n",
      "Model weights saved in outputs/checkpoint-2622/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-2622/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-2622/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-3933\n",
      "Configuration saved in outputs/checkpoint-3933/config.json\n",
      "Model weights saved in outputs/checkpoint-3933/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-3933/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-3933/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-5244\n",
      "Configuration saved in outputs/checkpoint-5244/config.json\n",
      "Model weights saved in outputs/checkpoint-5244/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-5244/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-5244/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-6555\n",
      "Configuration saved in outputs/checkpoint-6555/config.json\n",
      "Model weights saved in outputs/checkpoint-6555/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-6555/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-6555/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-7866\n",
      "Configuration saved in outputs/checkpoint-7866/config.json\n",
      "Model weights saved in outputs/checkpoint-7866/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-7866/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-7866/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-9177\n",
      "Configuration saved in outputs/checkpoint-9177/config.json\n",
      "Model weights saved in outputs/checkpoint-9177/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-9177/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-9177/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-10488\n",
      "Configuration saved in outputs/checkpoint-10488/config.json\n",
      "Model weights saved in outputs/checkpoint-10488/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-10488/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-10488/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-11799\n",
      "Configuration saved in outputs/checkpoint-11799/config.json\n",
      "Model weights saved in outputs/checkpoint-11799/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-11799/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-11799/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20971\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to outputs/checkpoint-13110\n",
      "Configuration saved in outputs/checkpoint-13110/config.json\n",
      "Model weights saved in outputs/checkpoint-13110/pytorch_model.bin\n",
      "tokenizer config file saved in outputs/checkpoint-13110/tokenizer_config.json\n",
      "Special tokens file saved in outputs/checkpoint-13110/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from outputs/checkpoint-6555 (score: 0.5250874161720276).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 34952\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1093' max='1093' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1093/1093 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "early stopping required metric_for_best_model, but did not find eval_loss so early stopping is disabled\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random, randrange\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "import torch\n",
    "\n",
    "def compute_metrics_binary(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    prediction_scores = torch.nn.functional.softmax(\n",
    "        torch.from_numpy(logits).double(), dim=-1).numpy() \n",
    "    # predictions = np.argmax(logits, axis=-1) #equivalent\n",
    "    predictions = np.argmax(prediction_scores, axis=-1)\n",
    "    return binary_metrics.compute(\n",
    "        predictions=predictions, \n",
    "        references=labels, \n",
    "        prediction_scores=prediction_scores[:,1] #taking only prediction percentage for the label 1\n",
    "    )\n",
    "    \n",
    "#TODO human_ensembl_regulatory dataset multilabel metrics\n",
    "def compute_metrics_multi(eval_preds):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for dataset_name, dataset_version in tqdm(DATASETS):\n",
    "    labels = sorted([x.stem for x in (benchmark_root / dataset_name / 'train').iterdir()])\n",
    "\n",
    "    tmp_dict = {}\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        for nlabel, label in enumerate(labels):\n",
    "            for f in (benchmark_root / dataset_name / split / label).glob('*.txt'):\n",
    "                txt = f.read_text()\n",
    "                if not DATASET_THINING or DATASET_THINING==1:\n",
    "                    tmp_dict[f\"{label} {f.stem}\"] = (split, nlabel, txt)\n",
    "                elif random() < DATASET_THINING:\n",
    "                    tmp_dict[f\"{label} {f.stem}\"] = (split, nlabel, txt)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(tmp_dict).T.rename(columns = {0: \"dset\", 1: \"cat\", 2: \"seq\"})\n",
    "\n",
    "    ds = Dataset.from_pandas(df)\n",
    "\n",
    "    tok_ds = ds.map(tok_func, batched=False, remove_columns=['__index_level_0__', 'seq'])\n",
    "    tok_ds = tok_ds.rename_columns({'cat':'labels'})\n",
    "\n",
    "    dds = DatasetDict({\n",
    "        'train': tok_ds.filter(lambda x: x[\"dset\"] == \"train\").remove_columns('dset'),\n",
    "        'test':  tok_ds.filter(lambda x: x[\"dset\"] == \"test\").remove_columns('dset')\n",
    "    })\n",
    "    train_valid_split = dds['train'].train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "    dds['train']=train_valid_split['train']\n",
    "    dds['valid']=train_valid_split['test']\n",
    "\n",
    "    compute_metrics = compute_metrics_binary if len(labels) == 2 else compute_metrics_multi\n",
    "\n",
    "    for _ in range(RUNS):\n",
    "        model_cls = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(labels))\n",
    "        if(RANDOMIZE_WEIGHTS):\n",
    "            # model_cls.init_weights() #Alternative\n",
    "            model_cls = AutoModelForSequenceClassification.from_config(model_cls.config)   \n",
    "            if(RESIZE_EMBEDDINGS):\n",
    "                model_cls.resize_token_embeddings(len(tokenizer))\n",
    "            \n",
    "        args = get_trainargs()\n",
    "        \n",
    "        trainer = Trainer(model_cls, args, train_dataset=dds['train'], eval_dataset=dds['valid'],\n",
    "                          tokenizer=tokenizer, compute_metrics=compute_metrics, \n",
    "                          callbacks=callbacks)\n",
    "        trainer.train()\n",
    "        trainer.evaluate(dds['test'], metric_key_prefix='test')\n",
    "        training_log = get_log_from_history(trainer.state.log_history, dataset_name=dataset_name)\n",
    "        outputs.append(training_log)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fckdKKcoYbFR"
   },
   "source": [
    "## Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q-d5WgMLTR-E"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_auroc_macro</th>\n",
       "      <th>test_auroc_weighted</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>min_valid_loss_epoch</th>\n",
       "      <th>min_valid_loss_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>demo_coding_vs_intergenomic_seqs</td>\n",
       "      <td>0.908920</td>\n",
       "      <td>0.909911</td>\n",
       "      <td>0.233794</td>\n",
       "      <td>0.900117</td>\n",
       "      <td>0.919920</td>\n",
       "      <td>0.969839</td>\n",
       "      <td>0.969839</td>\n",
       "      <td>0.971102</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'eval_loss': 0.22497177124023438, 'eval_accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demo_human_or_worm</td>\n",
       "      <td>0.955280</td>\n",
       "      <td>0.955677</td>\n",
       "      <td>0.117082</td>\n",
       "      <td>0.947265</td>\n",
       "      <td>0.964240</td>\n",
       "      <td>0.992608</td>\n",
       "      <td>0.992608</td>\n",
       "      <td>0.992931</td>\n",
       "      <td>3.0</td>\n",
       "      <td>{'eval_loss': 0.11374924331903458, 'eval_accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human_enhancers_cohn</td>\n",
       "      <td>0.742228</td>\n",
       "      <td>0.756625</td>\n",
       "      <td>0.524499</td>\n",
       "      <td>0.716602</td>\n",
       "      <td>0.801382</td>\n",
       "      <td>0.821101</td>\n",
       "      <td>0.821101</td>\n",
       "      <td>0.808286</td>\n",
       "      <td>4.0</td>\n",
       "      <td>{'eval_loss': 0.520462691783905, 'eval_accurac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human_enhancers_ensembl</td>\n",
       "      <td>0.843106</td>\n",
       "      <td>0.844666</td>\n",
       "      <td>0.365846</td>\n",
       "      <td>0.836351</td>\n",
       "      <td>0.853148</td>\n",
       "      <td>0.922144</td>\n",
       "      <td>0.922144</td>\n",
       "      <td>0.917993</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'eval_loss': 0.36180582642555237, 'eval_accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human_nontata_promoters</td>\n",
       "      <td>0.880894</td>\n",
       "      <td>0.879291</td>\n",
       "      <td>0.453799</td>\n",
       "      <td>0.979995</td>\n",
       "      <td>0.797355</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.946411</td>\n",
       "      <td>0.962192</td>\n",
       "      <td>7.0</td>\n",
       "      <td>{'eval_loss': 0.19008123874664307, 'eval_accur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human_ocr_ensembl</td>\n",
       "      <td>0.732404</td>\n",
       "      <td>0.748054</td>\n",
       "      <td>0.532017</td>\n",
       "      <td>0.706724</td>\n",
       "      <td>0.794518</td>\n",
       "      <td>0.813939</td>\n",
       "      <td>0.813939</td>\n",
       "      <td>0.801416</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'eval_loss': 0.5250874161720276, 'eval_accura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            dataset  test_acc   test_f1  test_loss  \\\n",
       "0  demo_coding_vs_intergenomic_seqs  0.908920  0.909911   0.233794   \n",
       "1                demo_human_or_worm  0.955280  0.955677   0.117082   \n",
       "2              human_enhancers_cohn  0.742228  0.756625   0.524499   \n",
       "3           human_enhancers_ensembl  0.843106  0.844666   0.365846   \n",
       "4           human_nontata_promoters  0.880894  0.879291   0.453799   \n",
       "5                 human_ocr_ensembl  0.732404  0.748054   0.532017   \n",
       "\n",
       "   test_precision  test_recall  test_auroc_macro  test_auroc_weighted  \\\n",
       "0        0.900117     0.919920          0.969839             0.969839   \n",
       "1        0.947265     0.964240          0.992608             0.992608   \n",
       "2        0.716602     0.801382          0.821101             0.821101   \n",
       "3        0.836351     0.853148          0.922144             0.922144   \n",
       "4        0.979995     0.797355          0.946411             0.946411   \n",
       "5        0.706724     0.794518          0.813939             0.813939   \n",
       "\n",
       "   test_pr_auc  min_valid_loss_epoch  \\\n",
       "0     0.971102                   4.0   \n",
       "1     0.992931                   3.0   \n",
       "2     0.808286                   4.0   \n",
       "3     0.917993                   5.0   \n",
       "4     0.962192                   7.0   \n",
       "5     0.801416                   5.0   \n",
       "\n",
       "                                  min_valid_loss_log  \n",
       "0  {'eval_loss': 0.22497177124023438, 'eval_accur...  \n",
       "1  {'eval_loss': 0.11374924331903458, 'eval_accur...  \n",
       "2  {'eval_loss': 0.520462691783905, 'eval_accurac...  \n",
       "3  {'eval_loss': 0.36180582642555237, 'eval_accur...  \n",
       "4  {'eval_loss': 0.19008123874664307, 'eval_accur...  \n",
       "5  {'eval_loss': 0.5250874161720276, 'eval_accura...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_df = pd.DataFrame(outputs)\n",
    "outputs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QXq1XNgFVKSn"
   },
   "outputs": [],
   "source": [
    "# outputs_df.groupby('dataset').agg({'accuracy' : ['mean', 'sem'], 'f1' : ['mean','sem'], 'train_runtime': ['mean', 'sem']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bP6TQPmYYlrb"
   },
   "outputs": [],
   "source": [
    "# saving outputs to csv file\n",
    "outputs_df.to_csv(OUTPUT_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Metrics_on_genomic_benchmarks_short_inputs_colab.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:myCudaCondaEnv]",
   "language": "python",
   "name": "conda-env-myCudaCondaEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01d30f8e4fde43fc8a3e4593d3932d47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "240183ed0048432e8f953293ea24bdce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "628aa13a3c8140be876e937c87d111be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1389c9d504e41b3bdc51a8805cfee5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb813c47be474313bb9bdbec8327ddaf",
       "IPY_MODEL_d33afcc4a33442d090d29ff1284e49ec",
       "IPY_MODEL_dd67380f41b548cc8882b31219ab36de"
      ],
      "layout": "IPY_MODEL_628aa13a3c8140be876e937c87d111be"
     }
    },
    "b59de534cb5b4d4486c6033c8f7bb4c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c04acfbcab1549e891f2320534af0400": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb813c47be474313bb9bdbec8327ddaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b59de534cb5b4d4486c6033c8f7bb4c2",
      "placeholder": "",
      "style": "IPY_MODEL_e3f54f15415a4a27bb77a7c06410db57",
      "value": "100%"
     }
    },
    "d33afcc4a33442d090d29ff1284e49ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01d30f8e4fde43fc8a3e4593d3932d47",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_240183ed0048432e8f953293ea24bdce",
      "value": 1
     }
    },
    "dd67380f41b548cc8882b31219ab36de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9ca594da9214565bf101c528d74fc3d",
      "placeholder": "",
      "style": "IPY_MODEL_c04acfbcab1549e891f2320534af0400",
      "value": " 1/1 [00:00&lt;00:00, 28.86it/s]"
     }
    },
    "e3f54f15415a4a27bb77a7c06410db57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e9ca594da9214565bf101c528d74fc3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
