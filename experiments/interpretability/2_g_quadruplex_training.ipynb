{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHIz512WllcJ"
   },
   "source": [
    "# DNABert Training for G-quafruplex classification\n",
    "\n",
    "---\n",
    "\n",
    "Used model: [armheb/DNA_bert_6](https://huggingface.co/armheb/DNA_bert_6?text=The+goal+of+life+is+%5BMASK%5D.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load initial libraries, models, data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbRnd7X7lqHM",
    "outputId": "91ad064d-e429-41c7-f60f-585bd870f335"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_ORIG_MODEL_NAME = 'armheb/DNA_bert_6'\n",
    "HF_MODEL_NAME = 'DNABert_K6_G_quad'\n",
    "# TOKENIZER = 'armheb/DNA_bert_6'\n",
    "\n",
    "TMP_DIR = './q_quad_dnabert'\n",
    "DATASET = 'roa7n/G_quad_DNA_tokenized_K6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff82c5b98c346db9b4c8be87349e4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "raWAmE4tllcM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained(TOKENIZER)\n",
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "c3bd270e75534d83b7f21027fca5b645",
      "38760edf77ab4da6815137aec5a1a7c2",
      "fce89f9021484783b47b7ebe446efb22",
      "e895dfa521254c4893abd2feda378265",
      "f43c369c08ab49cdb533997e20de6be5",
      "89a9c8cbc4c34828bb8822a34bc1c0bf",
      "26ef41898bb44269a2a3ce85880e73ac",
      "e28d267a1bf446dbb76e54a9b173fda1",
      "2b8b76702a3147ea871e397605a21d93",
      "99651468e75143b3b4262235bcb3b66b",
      "c7c2591b7c1c43639e61d831f146640f",
      "1a971bf9a67d460088126e32caf287c4",
      "095fd869a20f418eb8173b0a48005748",
      "e1c271234ea143dc8a0305965c7619b5",
      "26470925ac1b4f0e8997ff9719211369",
      "479371c84db74348a3e011934e4c103b",
      "038fc8559a5b45c39d690f6cc894891a",
      "834ccf0219c8435c9be43c18c829888e",
      "850e217b2a9a4e07932c19d002af648c",
      "b74b456daca640148f2a5a69b0eb09ff",
      "1cb98a8bff7d4b50bd1975b30c6f553c",
      "1619fb8d4fd04c5ea66dac4f5295a426",
      "1d1d200725dd4c998624d86abb669a25",
      "c60d4dbd27a3424aab695fd2c9078131",
      "6f23b9b936374ad6b7953923a72661fe",
      "c02b9de33e7041a6836eb585c98d032f",
      "46373e929c83484e8b9b4c85b99a809e",
      "73c1f8300fd147949ceaf96e633dd341",
      "38c9a16c6b4347df87763c63a3aedf8d",
      "39722837c0d54827ac7abb4c275d129b",
      "9aefc38bb2f046689cb05b13a5a6ee69",
      "dbb7e4188c644c648802840c1aff9047",
      "7a97be4613bc4a8f9e65bc436482dd28",
      "2244326009de490798e02746b75e511c",
      "66e56bdf3cb34fc1a41a01cf8a09fefd",
      "2ae2681b4cbd400188bb57cd24804828",
      "8f19bf5108d541ffad1ffaaa774e3ab0",
      "a32f5e2dd6a44733907dd3e3c81347f8",
      "30cfb5c6776d400cb9f9557c171ce963",
      "5295d509230848e9ad13dc570fe7039d",
      "4d6d1b3d92b049128a6fcc3baf364c5a",
      "57516610a7344c289f0683012994dfd3",
      "1fc71fe5a80f45beb2efbac199e1225f",
      "b7a7ba2dff1444b8b3a1fe66834dec8e",
      "d1d2547553df43d1b5d4d3b6a1784472",
      "79139a1f358b45b9abc4bfccf9a83ad1",
      "1ce4c656a6fc41e0b43ea752d4763d82",
      "f4cb71ac5bc749a8ae6ff68264418fc8",
      "2d22f91d9a5c43e5ae03f3f3da633537",
      "25d0b8b3440444628fea7c175f1a15ab",
      "6b79ba7402b540bf87b8ba82b49b750a",
      "6ecd6bf86e824c23b2f56c32a0bef91c",
      "f8d5d0aa2dc647efb876a83dc530a16a",
      "dc8c77da03654564a0643eff12af1514",
      "19c79ce6c38b49f590f781f0de30ebaa"
     ]
    },
    "id": "sUvdSBR_llcU",
    "outputId": "38ac8397-1e63-4a05-fe83-5ddef35ac682"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at armheb/DNA_bert_6 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at armheb/DNA_bert_6 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(4101, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(HF_ORIG_MODEL_NAME, num_labels=2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration roa7n--G_quad_DNA_tokenized_K6-226a4e1c505f34ab\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/roa7n___parquet/roa7n--G_quad_DNA_tokenized_K6-226a4e1c505f34ab/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba114041fc48455e9b0e231b31d350a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['seq', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 300000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['seq', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TCATAGACCCGGTCTCATATGACAAGGAGGGGCATGTCAGACAGTACCGAAGAAAACTGATCCTTTCCCTTTAGGGGGTCGGGGAACACGAAGGACCCACTCTGCTACGGGGTGGGACGAAGTCGGGGGGGAGGTACTCGACATGGGTGATAGGTTGGTCAGGGTTACCCTACTTGAACCATGGAGTCTACCTTTACGTC'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['seq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1560, 2129, 311, 1231, 815, 3248, 692, 2754, 2811, 3038, 3947, 3485, 1638, 2441, 1558, 2124, 289, 1143, 461, 1829, 3208, 532, 2113, 248, 980, 3908, 3332, 1027, 4093, 4070, 3980, 3618, 2171, 477, 1896, 3473, 1591, 2253, 808, 3218, 569, 2263, 847, 3376, 1201, 693, 2760, 2833, 3125, 197, 773, 3079, 14, 44, 161, 630, 2507, 1823, 3182, 426, 1690, 2651, 2399, 1391, 1454, 1706, 2714, 2649, 2392, 1364, 1348, 1284, 1028, 4098, 4091, 4064, 3956, 3524, 1796, 3073, 4085, 4039, 3853, 3111, 144, 561, 2229, 712, 2836, 3137, 247, 975, 3887, 3245, 679, 2702, 2603, 2206, 620, 2467, 1662, 2537, 1943, 3664, 2356, 1220, 772, 3074, 4092, 4068, 3972, 3585, 2039, 4048, 3889, 3253, 712, 2834, 3131, 224, 884, 3524, 1796, 3076, 4100, 4100, 4097, 4088, 4052, 3906, 3321, 983, 3918, 3371, 1184, 625, 2487, 1741, 2854, 3212, 548, 2180, 514, 2044, 4065, 3958, 3529, 1816, 3156, 322, 1274, 988, 3940, 3458, 1531, 2013, 3944, 3476, 1604, 2306, 1018, 4057, 3927, 3407, 1327, 1198, 681, 2711, 2638, 2346, 1180, 609, 2421, 1479, 1807, 3117, 166, 652, 2596, 2177, 504, 2002, 3899, 3294, 873, 3479, 1615, 2350, 1194, 666, 2649, 2391, 1360, 1330, 1211, 3]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/roa7n/DNABert_K6_G_quad into local empty directory.\n",
      "Using cuda_amp half precision backend\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.trainer.Trainer at 0x7f3d2f7edaf0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCHS = 25\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=TMP_DIR, \n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=BATCH_SIZE, \n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HF_MODEL_NAME,\n",
    "    hub_strategy=\"every_save\",\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    logging_strategy='epoch',\n",
    "    )\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/my-conda-envs/ml_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 300000\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 234375\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Couldn't find a Git repository in '/home/jovyan/jupyter_notebooks' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/roa7n/huggingface/ae91fca4e60b4022a02045f643bfe899\n",
      "\n",
      "Automatic Comet.ml online logging enabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234375' max='234375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234375/234375 13:02:25, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092700</td>\n",
       "      <td>0.081817</td>\n",
       "      <td>0.971950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068100</td>\n",
       "      <td>0.071439</td>\n",
       "      <td>0.975620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.060700</td>\n",
       "      <td>0.086300</td>\n",
       "      <td>0.973410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.078673</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.088243</td>\n",
       "      <td>0.975840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.044500</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.975240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.102395</td>\n",
       "      <td>0.975480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034500</td>\n",
       "      <td>0.110836</td>\n",
       "      <td>0.973890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.123502</td>\n",
       "      <td>0.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.134819</td>\n",
       "      <td>0.973040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.142740</td>\n",
       "      <td>0.973340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.146188</td>\n",
       "      <td>0.973790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.157013</td>\n",
       "      <td>0.973010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.972950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.191127</td>\n",
       "      <td>0.973540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.197747</td>\n",
       "      <td>0.973630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.199326</td>\n",
       "      <td>0.973200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.008300</td>\n",
       "      <td>0.217250</td>\n",
       "      <td>0.973550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.224195</td>\n",
       "      <td>0.973990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>0.224503</td>\n",
       "      <td>0.973240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.230579</td>\n",
       "      <td>0.973280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.241431</td>\n",
       "      <td>0.973740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.239394</td>\n",
       "      <td>0.973460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.241989</td>\n",
       "      <td>0.973630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.242375</td>\n",
       "      <td>0.973650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-1000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-1000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-1500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-1500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-2000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-2000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-2500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-2500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-3000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-3000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-3500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-3500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-4000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-4000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-4500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-4500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-5000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-5000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-5500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-5500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-6000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-6000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-6500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-6500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-7000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-7000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-7500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-7500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-8000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-8000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-8500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-8500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-9000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-9000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-8500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-9500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-9500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-10000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-10000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-10500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-10500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-11000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-11000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-11500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-11500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-12000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-12000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-12500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-12500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-13000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-13000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-13500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-13500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-14000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-14000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-14500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-14500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-15000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-15000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-15500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-15500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-16000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-16000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-16500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-16500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-17000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-17000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-17500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-17500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-18000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-18000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-18500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-18500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-18000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-19000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-19000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-19500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-19500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-20000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-20000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-20500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-20500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-21000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-21000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-21500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-21500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-22000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-22000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-22500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-22500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-23000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-23000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-23500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-23500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-24000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-24000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-24500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-24500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-25000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-25000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-25500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-25500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-26000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-26000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-26500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-26500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-27000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-27000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-27500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-27500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-28000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-28000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-27500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-28500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-28500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-29000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-29000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-29500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-29500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-30000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-30000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-30500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-30500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-31000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-31000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-31500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-31500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-32000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-32000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-32500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-32500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-33000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-33000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-33500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-33500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-34000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-34000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-34500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-34500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-35000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-35000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-35500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-35500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-36000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-36000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-36500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-36500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-37000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-37000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-37500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-37500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-37000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-38000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-38000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-38500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-38500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-39000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-39000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-39500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-39500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-40000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-40000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-40500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-40500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-41000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-41000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-41500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-41500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-42000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-42000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-42500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-42500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-43000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-43000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-43500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-43500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-44000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-44000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-44500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-44500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-45000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-45000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-45500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-45500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-46000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-46000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-46500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-46500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-46000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-47000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-47000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-47500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-47500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-48000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-48000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-48500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-48500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-49000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-49000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-49500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-49500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-50000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-50000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-50500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-50500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-50500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-51000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-51000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-51500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-51500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-52000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-52000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-52500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-52500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-53000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-53000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-53500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-53500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-53500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-54000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-54000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-54500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-54500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-54500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-55000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-55000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-55500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-55500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-55500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-56000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-56000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-55500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-56500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-56500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-56500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-57000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-57000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-57500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-57500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-57500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-58000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-58000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-58500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-58500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-58500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-59000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-59000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-59500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-59500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-59500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-60000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-60000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-60500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-60500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-60500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-61000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-61000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-61000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-61500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-61500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-61500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-62000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-62000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-62500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-62500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-62500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-63000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-63000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-63000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-63500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-63500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-63500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-64000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-64000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-64000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-64500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-64500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-64500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-65000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-65000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-65000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-65500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-65500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-65500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-65000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-66000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-66000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-66000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-66500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-66500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-66500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-67000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-67000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-67000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-67500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-67500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-67500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-68000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-68000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-68000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-68500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-68500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-68500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-68000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-69000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-69000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-69000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-69500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-69500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-69500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-70000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-70000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-70500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-70500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-70500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-71000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-71000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-71000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-71500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-71500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-71500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-71000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-72000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-72000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-72000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-72500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-72500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-72500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-72000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-73000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-73000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-73000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-73500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-73500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-73500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-74000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-74000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-74000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-74500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-74500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-74500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-75000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-75000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-75000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-74500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-75500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-75500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-75500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-76000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-76000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-76000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-75500] due to args.save_total_limit\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-77000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-77000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-77000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-77500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-77500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-77500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-77000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-78000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-78000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-78000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-78500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-78500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-78500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-78000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-79000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-79000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-79000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-79500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-79500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-79500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-79000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-80000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-80000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-80500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-80500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-80500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-81000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-81000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-81000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-81500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-81500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-81500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-81000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-82000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-82000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-82000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-81500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-82500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-82500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-82500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-83000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-83000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-83000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-83500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-83500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-83500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-84000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-84000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-84000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-83500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-84500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-84500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-84500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-85000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-85000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-85000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-85500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-85500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-85500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-86000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-86000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-86000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-86500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-86500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-86500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-86000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-87000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-87000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-87000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-87500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-87500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-87500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-87000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-88000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-88000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-88000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-88500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-88500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-88500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-88000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-89000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-89000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-89000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-88500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-89500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-89500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-89500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-89000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-90000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-90000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-90500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-90500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-90500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-90000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-91000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-91000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-91000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-90500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-91500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-91500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-91500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-91000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-92000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-92000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-92000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-92500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-92500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-92500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-92000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-93000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-93000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-93000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-93500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-93500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-93500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-93000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-94000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-94000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-94000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-94500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-94500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-94500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-94000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-95000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-95000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-95000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-94500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-95500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-95500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-95500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-95000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-96000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-96000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-96000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-96500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-96500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-96500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-96000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-97000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-97000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-97000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-97500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-97500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-97500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-97000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-98000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-98000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-98000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-97500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-98500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-98500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-98500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-98000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-99000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-99000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-99000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-99500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-99500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-99500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-99000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-100000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-100000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-100500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-100500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-100500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-100000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-101000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-101000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-101000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-101500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-101500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-101500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-101000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-102000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-102000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-102000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-102500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-102500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-102500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-102000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-103000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-103000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-103000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-102500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-103500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-103500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-103500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-103000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-104000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-104000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-104000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-104500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-104500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-104500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-104000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-105000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-105000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-105000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-104500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-105500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-105500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-105500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-105000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-106000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-106000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-106000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-106500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-106500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-106500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-106000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-107000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-107000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-107000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-107500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-107500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-107500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-107000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-108000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-108000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-108000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-107500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-108500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-108500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-108500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-108000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-109000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-109000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-109000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-109500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-109500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-109500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-109000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-110000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-110000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-110500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-110500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-110500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-110000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-111000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-111000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-111000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-111500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-111500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-111500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-111000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-112000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-112000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-112000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-111500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-112500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-112500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-112500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-112000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-113000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-113000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-113000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-113500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-113500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-113500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-113000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-114000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-114000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-114000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-113500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-114500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-114500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-114500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-114000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-115000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-115000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-115000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-114500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-115500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-115500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-115500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-115000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-116000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-116000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-116000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-116500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-116500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-116500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-116000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-117000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-117000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-117000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-117500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-117500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-117500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-117000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-118000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-118000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-118000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-117500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-118500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-118500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-118500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-118000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-119000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-119000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-119000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-118500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-119500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-119500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-119500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-119000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-120000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-120000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-120500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-120500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-120500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-121000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-121000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-121000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-120500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-121500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-121500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-121500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-121000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-122000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-122000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-122000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-122500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-122500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-122500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-122000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-123000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-123000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-123000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-123500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-123500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-123500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-123000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-124000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-124000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-124000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-124500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-124500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-124500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-124000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-125000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-125000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-125000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-125500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-125500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-125500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-125000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-126000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-126000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-126000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-126500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-126500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-126500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-126000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-127000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-127000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-127000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-127500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-127500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-127500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-127000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-128000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-128000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-128000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-127500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-128500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-128500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-128500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-128000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-129000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-129000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-129000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-128500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-129500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-129500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-129500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-129000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-130000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-130000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-130000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-129500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-130500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-130500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-130500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-130000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-131000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-131000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-131000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-130500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-131500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-131500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-131500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-131000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-132000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-132000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-132000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-131500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-132500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-132500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-132500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-132000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-133000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-133000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-133000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-132500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-133500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-133500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-133500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-133000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-134000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-134000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-134000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-133500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-134500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-134500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-134500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-134000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-135000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-135000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-135000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-134500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-135500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-135500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-135500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-135000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-136000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-136000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-136000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-135500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-136500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-136500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-136500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-136000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-137000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-137000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-137000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-136500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-137500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-137500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-137500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-137000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-138000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-138000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-138000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-137500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-138500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-138500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-138500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-138000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-139000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-139000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-139000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-138500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-139500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-139500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-139500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-139000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-140000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-140000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-140000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-139500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-140500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-140500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-140500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-140000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-141000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-141000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-141000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-140500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-141500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-141500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-141500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-141000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-142000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-142000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-142000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-141500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-142500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-142500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-142500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-142000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-143000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-143000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-143000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-142500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-143500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-143500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-143500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-143000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-144000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-144000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-144000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-143500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-144500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-144500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-144500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-144000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-145000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-145000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-145000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-144500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-145500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-145500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-145500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-145000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-146000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-146000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-146000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-145500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-146500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-146500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-146500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-146000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-147000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-147000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-147000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-146500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-147500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-147500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-147500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-147000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-148000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-148000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-148000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-147500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-148500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-148500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-148500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-148000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-149000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-149000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-149000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-148500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-149500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-149500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-149500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-149000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-150000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-150000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-149500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-150500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-150500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-150500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-150000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-151000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-151000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-151000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-150500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-151500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-151500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-151500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-151000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-152000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-152000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-152000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-151500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-152500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-152500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-152500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-152000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-153000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-153000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-153000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-152500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-153500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-153500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-153500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-153000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-154000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-154000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-154000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-153500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-157500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-157500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-157500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-157000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-158000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-158000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-158000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-157500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-158500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-158500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-158500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-158000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-159500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-159500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-159500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-159000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-160000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-160000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-160000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-159500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-160500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-160500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-160500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-160000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-161000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-161000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-161000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-160500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-161500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-161500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-161500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-161000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-162000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-162000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-162000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-161500] due to args.save_total_limit\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-164500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-165500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-165500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-165500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-165000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-166500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-166500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-166500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-166000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-169500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-169500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-169500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-169000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-170000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-170000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-170000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-169500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-170500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-170500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-170500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-170000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-171000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-171000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-171000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-170500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-174500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-174500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-174500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-174000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-175000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-175000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-175000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-174500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-175500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-175500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-175500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-175000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-178500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-178500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-178500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-178000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-179000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-179000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-179000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-178500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-179500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-179500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-179500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-179000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-180000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-180000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-180000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-179500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-180500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-180500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-180500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-180000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-181000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-181000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-181000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-180500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-181500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-181500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-181500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-181000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-182000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-182000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-182000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-181500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-182500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-182500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-182500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-182000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-183000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-183000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-183000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-182500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-183500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-183500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-183500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-183000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-184000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-184000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-184000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-183500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-184500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-184500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-184500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-184000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-185000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-185000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-185000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-184500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-185500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-185500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-185500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-185000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-186000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-186000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-186000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-185500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-186500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-186500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-186500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-186000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-187000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-187000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-187000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-186500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-187500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-187500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-187500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-187000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-188000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-188000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-188000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-187500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-188500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-188500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-188500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-188000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-189000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-189000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-189000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-188500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-189500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-189500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-189500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-189000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-190000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-190000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-190000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-189500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-190500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-190500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-190500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-190000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-191000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-191000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-191000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-190500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-191500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-191500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-191500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-191000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-192000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-192000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-192000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-191500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-192500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-192500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-192500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-192000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-193000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-193000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-193000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-192500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-193500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-193500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-193500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-193000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-194000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-194000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-194000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-193500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-194500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-194500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-194500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-194000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-195000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-195000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-195000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-194500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-195500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-195500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-195500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-195000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-196000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-196000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-196000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-195500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-196500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-196500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-196500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-196000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-197000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-197000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-197000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-196500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-197500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-197500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-197500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-197000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-198000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-198000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-198000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-197500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-198500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-198500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-198500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-198000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-199000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-199000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-199000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-198500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-199500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-199500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-199500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-199000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-200000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-200000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-200000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-199500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-200500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-200500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-200500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-200000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-201000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-201000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-201000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-200500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-201500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-201500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-201500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-201000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-202000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-202000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-202000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-201500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-202500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-202500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-202500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-202000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-203000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-203000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-203000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-202500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-203500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-203500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-203500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-203000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-204000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-204000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-204000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-203500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-204500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-204500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-204500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-204000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-205000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-205000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-205000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-204500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-205500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-205500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-205500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-205000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-206000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-206000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-206000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-205500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-206500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-206500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-206500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-206000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-207000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-207000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-207000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-206500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-207500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-207500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-207500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-207000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-208000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-208000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-208000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-207500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-208500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-208500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-208500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-208000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-209000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-209000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-209000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-208500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-209500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-209500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-209500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-209000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-210000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-210000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-210000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-209500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-210500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-210500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-210500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-210000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-211000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-211000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-211000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-210500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-211500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-211500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-211500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-211000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-212000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-212000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-212000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-211500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-212500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-212500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-212500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-212000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-213000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-213000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-213000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-212500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-213500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-213500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-213500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-213000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-214000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-214000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-214000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-213500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-214500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-214500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-214500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-214000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-215000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-215000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-215000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-214500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-215500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-215500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-215500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-215000] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-216000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-216000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-216000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-215500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-216500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-216500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-216500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-216000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-217000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-217000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-217000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-216500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-217500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-217500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-217500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-217000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-218000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-218000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-218000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-217500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-218500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-218500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-218500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-218000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-219000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-219000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-219000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-218500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-219500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-219500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-219500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-219000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-220000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-220000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-220000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-219500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-220500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-220500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-220500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-220000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-221000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-221000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-221000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-220500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-221500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-221500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-221500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-221000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-222000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-222000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-222000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-221500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-222500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-222500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-222500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-222000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-223000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-223000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-223000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-222500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-223500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-223500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-223500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-223000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-224000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-224000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-224000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-223500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-224500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-224500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-224500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-224000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-225000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-225000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-225000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-224500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-225500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-225500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-225500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-225000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-226000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-226000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-226000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-225500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-226500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-226500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-226500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-226000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-227000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-227000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-227000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-226500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-227500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-227500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-227500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-227000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-228000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-228000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-228000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-227500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-228500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-228500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-228500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-228000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-229000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-229000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-229000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-228500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-229500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-229500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-229500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-229000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-230000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-230000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-230000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-229500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-230500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-230500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-230500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-230000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-231000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-231000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-231000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-230500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-231500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-231500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-231500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-231000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-232000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-232000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-232000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-231500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-232500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-232500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-232500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-232000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-233000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-233000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-233000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-232500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-233500\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-233500/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-233500/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-233000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./q_quad_dnabert/checkpoint-234000\n",
      "Configuration saved in ./q_quad_dnabert/checkpoint-234000/config.json\n",
      "Model weights saved in ./q_quad_dnabert/checkpoint-234000/pytorch_model.bin\n",
      "Deleting older checkpoint [q_quad_dnabert/checkpoint-233500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 100000\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/roa7n/huggingface/ae91fca4e60b4022a02045f643bfe899\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     epoch [51]                   : (1.0, 25.0)\n",
      "COMET INFO:     eval_accuracy [25]           : (0.97195, 0.97584)\n",
      "COMET INFO:     eval_loss [25]               : (0.07143893092870712, 0.2423752099275589)\n",
      "COMET INFO:     eval_runtime [25]            : (140.7478, 234.5543)\n",
      "COMET INFO:     eval_samples_per_second [25] : (426.341, 710.491)\n",
      "COMET INFO:     eval_steps_per_second [25]   : (13.323, 22.203)\n",
      "COMET INFO:     learning_rate [25]           : (4.010666666666667e-09, 9.600213333333334e-06)\n",
      "COMET INFO:     loss [25]                    : (0.0032, 0.0927)\n",
      "COMET INFO:     total_flos                   : 7.5927067245e+17\n",
      "COMET INFO:     train_loss                   : 0.026095547005208334\n",
      "COMET INFO:     train_runtime                : 46946.2681\n",
      "COMET INFO:     train_samples_per_second     : 159.757\n",
      "COMET INFO:     train_steps_per_second       : 4.992\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     args/_n_gpu                             : 1\n",
      "COMET INFO:     args/_no_sync_in_gradient_accumulation  : True\n",
      "COMET INFO:     args/_setup_devices                     : cuda:0\n",
      "COMET INFO:     args/adafactor                          : False\n",
      "COMET INFO:     args/adam_beta1                         : 0.9\n",
      "COMET INFO:     args/adam_beta2                         : 0.999\n",
      "COMET INFO:     args/adam_epsilon                       : 1e-08\n",
      "COMET INFO:     args/auto_find_batch_size               : False\n",
      "COMET INFO:     args/bf16                               : False\n",
      "COMET INFO:     args/bf16_full_eval                     : False\n",
      "COMET INFO:     args/data_seed                          : None\n",
      "COMET INFO:     args/dataloader_drop_last               : False\n",
      "COMET INFO:     args/dataloader_num_workers             : 0\n",
      "COMET INFO:     args/dataloader_pin_memory              : True\n",
      "COMET INFO:     args/ddp_bucket_cap_mb                  : None\n",
      "COMET INFO:     args/ddp_find_unused_parameters         : None\n",
      "COMET INFO:     args/ddp_timeout                        : 1800\n",
      "COMET INFO:     args/ddp_timeout_delta                  : 0:30:00\n",
      "COMET INFO:     args/debug                              : []\n",
      "COMET INFO:     args/deepspeed                          : None\n",
      "COMET INFO:     args/device                             : cuda:0\n",
      "COMET INFO:     args/disable_tqdm                       : False\n",
      "COMET INFO:     args/do_eval                            : True\n",
      "COMET INFO:     args/do_predict                         : False\n",
      "COMET INFO:     args/do_train                           : False\n",
      "COMET INFO:     args/eval_accumulation_steps            : None\n",
      "COMET INFO:     args/eval_batch_size                    : 32\n",
      "COMET INFO:     args/eval_delay                         : 0\n",
      "COMET INFO:     args/eval_steps                         : None\n",
      "COMET INFO:     args/evaluation_strategy                : IntervalStrategy.EPOCH\n",
      "COMET INFO:     args/fp16                               : True\n",
      "COMET INFO:     args/fp16_backend                       : auto\n",
      "COMET INFO:     args/fp16_full_eval                     : False\n",
      "COMET INFO:     args/fp16_opt_level                     : O1\n",
      "COMET INFO:     args/framework                          : pt\n",
      "COMET INFO:     args/fsdp                               : []\n",
      "COMET INFO:     args/fsdp_min_num_params                : 0\n",
      "COMET INFO:     args/fsdp_transformer_layer_cls_to_wrap : None\n",
      "COMET INFO:     args/full_determinism                   : False\n",
      "COMET INFO:     args/gradient_accumulation_steps        : 1\n",
      "COMET INFO:     args/gradient_checkpointing             : False\n",
      "COMET INFO:     args/greater_is_better                  : None\n",
      "COMET INFO:     args/group_by_length                    : False\n",
      "COMET INFO:     args/half_precision_backend             : cuda_amp\n",
      "COMET INFO:     args/hub_model_id                       : DNABert_K6_G_quad\n",
      "COMET INFO:     args/hub_private_repo                   : False\n",
      "COMET INFO:     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "COMET INFO:     args/hub_token                          : None\n",
      "COMET INFO:     args/ignore_data_skip                   : False\n",
      "COMET INFO:     args/include_inputs_for_metrics         : False\n",
      "COMET INFO:     args/jit_mode_eval                      : False\n",
      "COMET INFO:     args/label_names                        : None\n",
      "COMET INFO:     args/label_smoothing_factor             : 0.0\n",
      "COMET INFO:     args/learning_rate                      : 1e-05\n",
      "COMET INFO:     args/length_column_name                 : length\n",
      "COMET INFO:     args/load_best_model_at_end             : False\n",
      "COMET INFO:     args/local_process_index                : 0\n",
      "COMET INFO:     args/local_rank                         : -1\n",
      "COMET INFO:     args/log_level                          : -1\n",
      "COMET INFO:     args/log_level_replica                  : -1\n",
      "COMET INFO:     args/log_on_each_node                   : True\n",
      "COMET INFO:     args/logging_dir                        : ./q_quad_dnabert/runs/Oct27_10-29-51_jupyter-xsramkov\n",
      "COMET INFO:     args/logging_first_step                 : False\n",
      "COMET INFO:     args/logging_nan_inf_filter             : True\n",
      "COMET INFO:     args/logging_steps                      : 500\n",
      "COMET INFO:     args/logging_strategy                   : IntervalStrategy.EPOCH\n",
      "COMET INFO:     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "COMET INFO:     args/max_grad_norm                      : 1.0\n",
      "COMET INFO:     args/max_steps                          : -1\n",
      "COMET INFO:     args/metric_for_best_model              : None\n",
      "COMET INFO:     args/mp_parameters                      : \n",
      "COMET INFO:     args/n_gpu                              : 1\n",
      "COMET INFO:     args/no_cuda                            : False\n",
      "COMET INFO:     args/num_train_epochs                   : 25\n",
      "COMET INFO:     args/optim                              : OptimizerNames.ADAMW_HF\n",
      "COMET INFO:     args/output_dir                         : ./q_quad_dnabert\n",
      "COMET INFO:     args/overwrite_output_dir               : True\n",
      "COMET INFO:     args/parallel_mode                      : ParallelMode.NOT_PARALLEL\n",
      "COMET INFO:     args/past_index                         : -1\n",
      "COMET INFO:     args/per_device_eval_batch_size         : 32\n",
      "COMET INFO:     args/per_device_train_batch_size        : 32\n",
      "COMET INFO:     args/per_gpu_eval_batch_size            : None\n",
      "COMET INFO:     args/per_gpu_train_batch_size           : None\n",
      "COMET INFO:     args/place_model_on_device              : True\n",
      "COMET INFO:     args/prediction_loss_only               : False\n",
      "COMET INFO:     args/process_index                      : 0\n",
      "COMET INFO:     args/push_to_hub                        : True\n",
      "COMET INFO:     args/push_to_hub_model_id               : None\n",
      "COMET INFO:     args/push_to_hub_organization           : None\n",
      "COMET INFO:     args/push_to_hub_token                  : None\n",
      "COMET INFO:     args/ray_scope                          : last\n",
      "COMET INFO:     args/remove_unused_columns              : True\n",
      "COMET INFO:     args/report_to                          : ['comet_ml']\n",
      "COMET INFO:     args/resume_from_checkpoint             : None\n",
      "COMET INFO:     args/run_name                           : ./q_quad_dnabert\n",
      "COMET INFO:     args/save_on_each_node                  : False\n",
      "COMET INFO:     args/save_steps                         : 500\n",
      "COMET INFO:     args/save_strategy                      : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/save_total_limit                   : 1\n",
      "COMET INFO:     args/seed                               : 42\n",
      "COMET INFO:     args/sharded_ddp                        : []\n",
      "COMET INFO:     args/should_log                         : True\n",
      "COMET INFO:     args/should_save                        : True\n",
      "COMET INFO:     args/skip_memory_metrics                : True\n",
      "COMET INFO:     args/tf32                               : None\n",
      "COMET INFO:     args/torchdynamo                        : None\n",
      "COMET INFO:     args/tpu_metrics_debug                  : False\n",
      "COMET INFO:     args/tpu_num_cores                      : None\n",
      "COMET INFO:     args/train_batch_size                   : 32\n",
      "COMET INFO:     args/use_ipex                           : False\n",
      "COMET INFO:     args/use_legacy_prediction_loop         : False\n",
      "COMET INFO:     args/use_mps_device                     : False\n",
      "COMET INFO:     args/warmup_ratio                       : 0.0\n",
      "COMET INFO:     args/warmup_steps                       : 0\n",
      "COMET INFO:     args/weight_decay                       : 0.0\n",
      "COMET INFO:     args/world_size                         : 1\n",
      "COMET INFO:     args/xpu_backend                        : None\n",
      "COMET INFO:     config/_auto_class                      : None\n",
      "COMET INFO:     config/_commit_hash                     : a79a8fd96ad172f964a4dbef3f4d7545a5034baa\n",
      "COMET INFO:     config/_name_or_path                    : armheb/DNA_bert_6\n",
      "COMET INFO:     config/add_cross_attention              : False\n",
      "COMET INFO:     config/architectures                    : ['BertForMaskedLM']\n",
      "COMET INFO:     config/attention_probs_dropout_prob     : 0.1\n",
      "COMET INFO:     config/attribute_map                    : {}\n",
      "COMET INFO:     config/bad_words_ids                    : None\n",
      "COMET INFO:     config/bos_token_id                     : 0\n",
      "COMET INFO:     config/chunk_size_feed_forward          : 0\n",
      "COMET INFO:     config/classifier_dropout               : None\n",
      "COMET INFO:     config/cross_attention_hidden_size      : None\n",
      "COMET INFO:     config/decoder_start_token_id           : None\n",
      "COMET INFO:     config/diversity_penalty                : 0.0\n",
      "COMET INFO:     config/do_sample                        : False\n",
      "COMET INFO:     config/early_stopping                   : False\n",
      "COMET INFO:     config/encoder_no_repeat_ngram_size     : 0\n",
      "COMET INFO:     config/eos_token_id                     : None\n",
      "COMET INFO:     config/eos_token_ids                    : 0\n",
      "COMET INFO:     config/exponential_decay_length_penalty : None\n",
      "COMET INFO:     config/finetuning_task                  : None\n",
      "COMET INFO:     config/forced_bos_token_id              : None\n",
      "COMET INFO:     config/forced_eos_token_id              : None\n",
      "COMET INFO:     config/hidden_act                       : gelu\n",
      "COMET INFO:     config/hidden_dropout_prob              : 0.1\n",
      "COMET INFO:     config/hidden_size                      : 768\n",
      "COMET INFO:     config/id2label                         : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "COMET INFO:     config/initializer_range                : 0.02\n",
      "COMET INFO:     config/intermediate_size                : 3072\n",
      "COMET INFO:     config/is_composition                   : False\n",
      "COMET INFO:     config/is_decoder                       : False\n",
      "COMET INFO:     config/is_encoder_decoder               : False\n",
      "COMET INFO:     config/label2id                         : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "COMET INFO:     config/layer_norm_eps                   : 1e-12\n",
      "COMET INFO:     config/length_penalty                   : 1.0\n",
      "COMET INFO:     config/max_length                       : 20\n",
      "COMET INFO:     config/max_position_embeddings          : 512\n",
      "COMET INFO:     config/min_length                       : 0\n",
      "COMET INFO:     config/model_type                       : bert\n",
      "COMET INFO:     config/name_or_path                     : armheb/DNA_bert_6\n",
      "COMET INFO:     config/no_repeat_ngram_size             : 0\n",
      "COMET INFO:     config/num_attention_heads              : 12\n",
      "COMET INFO:     config/num_beam_groups                  : 1\n",
      "COMET INFO:     config/num_beams                        : 1\n",
      "COMET INFO:     config/num_hidden_layers                : 12\n",
      "COMET INFO:     config/num_labels                       : 2\n",
      "COMET INFO:     config/num_return_sequences             : 1\n",
      "COMET INFO:     config/num_rnn_layer                    : 1\n",
      "COMET INFO:     config/output_attentions                : False\n",
      "COMET INFO:     config/output_hidden_states             : False\n",
      "COMET INFO:     config/output_past                      : True\n",
      "COMET INFO:     config/output_scores                    : False\n",
      "COMET INFO:     config/pad_token_id                     : 0\n",
      "COMET INFO:     config/position_embedding_type          : absolute\n",
      "COMET INFO:     config/prefix                           : None\n",
      "COMET INFO:     config/problem_type                     : None\n",
      "COMET INFO:     config/pruned_heads                     : {}\n",
      "COMET INFO:     config/remove_invalid_values            : False\n",
      "COMET INFO:     config/repetition_penalty               : 1.0\n",
      "COMET INFO:     config/return_dict                      : True\n",
      "COMET INFO:     config/return_dict_in_generate          : False\n",
      "COMET INFO:     config/rnn                              : lstm\n",
      "COMET INFO:     config/rnn_dropout                      : 0.0\n",
      "COMET INFO:     config/rnn_hidden                       : 768\n",
      "COMET INFO:     config/sep_token_id                     : None\n",
      "COMET INFO:     config/split                            : 10\n",
      "COMET INFO:     config/task_specific_params             : None\n",
      "COMET INFO:     config/temperature                      : 1.0\n",
      "COMET INFO:     config/tf_legacy_loss                   : False\n",
      "COMET INFO:     config/tie_encoder_decoder              : False\n",
      "COMET INFO:     config/tie_word_embeddings              : True\n",
      "COMET INFO:     config/tokenizer_class                  : None\n",
      "COMET INFO:     config/top_k                            : 50\n",
      "COMET INFO:     config/top_p                            : 1.0\n",
      "COMET INFO:     config/torch_dtype                      : None\n",
      "COMET INFO:     config/torchscript                      : False\n",
      "COMET INFO:     config/transformers_version             : None\n",
      "COMET INFO:     config/type_vocab_size                  : 2\n",
      "COMET INFO:     config/typical_p                        : 1.0\n",
      "COMET INFO:     config/use_bfloat16                     : False\n",
      "COMET INFO:     config/use_cache                        : True\n",
      "COMET INFO:     config/use_return_dict                  : True\n",
      "COMET INFO:     config/vocab_size                       : 4101\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-environment-definition : 1\n",
      "COMET INFO:     conda-info                   : 1\n",
      "COMET INFO:     conda-specification          : 1\n",
      "COMET INFO:     environment details          : 1\n",
      "COMET INFO:     filename                     : 1\n",
      "COMET INFO:     installed packages           : 1\n",
      "COMET INFO:     model graph                  : 1\n",
      "COMET INFO:     notebook                     : 1\n",
      "COMET INFO:     os packages                  : 1\n",
      "COMET INFO:     source_code                  : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET INFO: Uploading metrics, params, and assets to Comet before program termination (may take several seconds)\n",
      "COMET INFO: The Python SDK has 3600 seconds to finish before aborting...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=234375, training_loss=0.026095547005208334, metrics={'train_runtime': 46946.2681, 'train_samples_per_second': 159.757, 'train_steps_per_second': 4.992, 'total_flos': 7.5927067245e+17, 'train_loss': 0.026095547005208334, 'epoch': 25.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./q_quad_dnabert\n",
      "Configuration saved in ./q_quad_dnabert/config.json\n",
      "Model weights saved in ./q_quad_dnabert/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b453d52ecd43c3b25296b2ad70e53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/340M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/roa7n/DNABert_K6_G_quad\n",
      "   3f2e738..e88455b  main -> main\n",
      "\n",
      "Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.97365}]}\n",
      "To https://huggingface.co/roa7n/DNABert_K6_G_quad\n",
      "   e88455b..b2fc072  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/roa7n/DNABert_K6_G_quad/commit/e88455b32aec4589b54728808f7402cd81d0df9b'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(HF_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(HF_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:ml_env]",
   "language": "python",
   "name": "conda-env-ml_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "038fc8559a5b45c39d690f6cc894891a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "095fd869a20f418eb8173b0a48005748": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_038fc8559a5b45c39d690f6cc894891a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_834ccf0219c8435c9be43c18c829888e",
      "value": "Downloading: 100%"
     }
    },
    "1619fb8d4fd04c5ea66dac4f5295a426": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19c79ce6c38b49f590f781f0de30ebaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a971bf9a67d460088126e32caf287c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_095fd869a20f418eb8173b0a48005748",
       "IPY_MODEL_e1c271234ea143dc8a0305965c7619b5",
       "IPY_MODEL_26470925ac1b4f0e8997ff9719211369"
      ],
      "layout": "IPY_MODEL_479371c84db74348a3e011934e4c103b"
     }
    },
    "1cb98a8bff7d4b50bd1975b30c6f553c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ce4c656a6fc41e0b43ea752d4763d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ecd6bf86e824c23b2f56c32a0bef91c",
      "max": 40,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8d5d0aa2dc647efb876a83dc530a16a",
      "value": 40
     }
    },
    "1d1d200725dd4c998624d86abb669a25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c60d4dbd27a3424aab695fd2c9078131",
       "IPY_MODEL_6f23b9b936374ad6b7953923a72661fe",
       "IPY_MODEL_c02b9de33e7041a6836eb585c98d032f"
      ],
      "layout": "IPY_MODEL_46373e929c83484e8b9b4c85b99a809e"
     }
    },
    "1fc71fe5a80f45beb2efbac199e1225f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2244326009de490798e02746b75e511c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66e56bdf3cb34fc1a41a01cf8a09fefd",
       "IPY_MODEL_2ae2681b4cbd400188bb57cd24804828",
       "IPY_MODEL_8f19bf5108d541ffad1ffaaa774e3ab0"
      ],
      "layout": "IPY_MODEL_a32f5e2dd6a44733907dd3e3c81347f8"
     }
    },
    "25d0b8b3440444628fea7c175f1a15ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26470925ac1b4f0e8997ff9719211369": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cb98a8bff7d4b50bd1975b30c6f553c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1619fb8d4fd04c5ea66dac4f5295a426",
      "value": " 359M/359M [00:16&lt;00:00, 32.3MB/s]"
     }
    },
    "26ef41898bb44269a2a3ce85880e73ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ae2681b4cbd400188bb57cd24804828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d6d1b3d92b049128a6fcc3baf364c5a",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_57516610a7344c289f0683012994dfd3",
      "value": 112
     }
    },
    "2b8b76702a3147ea871e397605a21d93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d22f91d9a5c43e5ae03f3f3da633537": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30cfb5c6776d400cb9f9557c171ce963": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38760edf77ab4da6815137aec5a1a7c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89a9c8cbc4c34828bb8822a34bc1c0bf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_26ef41898bb44269a2a3ce85880e73ac",
      "value": "Downloading: 100%"
     }
    },
    "38c9a16c6b4347df87763c63a3aedf8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39722837c0d54827ac7abb4c275d129b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "46373e929c83484e8b9b4c85b99a809e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "479371c84db74348a3e011934e4c103b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d6d1b3d92b049128a6fcc3baf364c5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5295d509230848e9ad13dc570fe7039d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57516610a7344c289f0683012994dfd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "66e56bdf3cb34fc1a41a01cf8a09fefd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30cfb5c6776d400cb9f9557c171ce963",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5295d509230848e9ad13dc570fe7039d",
      "value": "Downloading: 100%"
     }
    },
    "6b79ba7402b540bf87b8ba82b49b750a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ecd6bf86e824c23b2f56c32a0bef91c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f23b9b936374ad6b7953923a72661fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39722837c0d54827ac7abb4c275d129b",
      "max": 28703,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9aefc38bb2f046689cb05b13a5a6ee69",
      "value": 28703
     }
    },
    "73c1f8300fd147949ceaf96e633dd341": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79139a1f358b45b9abc4bfccf9a83ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25d0b8b3440444628fea7c175f1a15ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6b79ba7402b540bf87b8ba82b49b750a",
      "value": "Downloading: 100%"
     }
    },
    "7a97be4613bc4a8f9e65bc436482dd28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "834ccf0219c8435c9be43c18c829888e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "850e217b2a9a4e07932c19d002af648c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89a9c8cbc4c34828bb8822a34bc1c0bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f19bf5108d541ffad1ffaaa774e3ab0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fc71fe5a80f45beb2efbac199e1225f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b7a7ba2dff1444b8b3a1fe66834dec8e",
      "value": " 112/112 [00:00&lt;00:00, 3.42kB/s]"
     }
    },
    "99651468e75143b3b4262235bcb3b66b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aefc38bb2f046689cb05b13a5a6ee69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a32f5e2dd6a44733907dd3e3c81347f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b74b456daca640148f2a5a69b0eb09ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b7a7ba2dff1444b8b3a1fe66834dec8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c02b9de33e7041a6836eb585c98d032f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbb7e4188c644c648802840c1aff9047",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7a97be4613bc4a8f9e65bc436482dd28",
      "value": " 28.7k/28.7k [00:00&lt;00:00, 456kB/s]"
     }
    },
    "c3bd270e75534d83b7f21027fca5b645": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38760edf77ab4da6815137aec5a1a7c2",
       "IPY_MODEL_fce89f9021484783b47b7ebe446efb22",
       "IPY_MODEL_e895dfa521254c4893abd2feda378265"
      ],
      "layout": "IPY_MODEL_f43c369c08ab49cdb533997e20de6be5"
     }
    },
    "c60d4dbd27a3424aab695fd2c9078131": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73c1f8300fd147949ceaf96e633dd341",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_38c9a16c6b4347df87763c63a3aedf8d",
      "value": "Downloading: 100%"
     }
    },
    "c7c2591b7c1c43639e61d831f146640f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1d2547553df43d1b5d4d3b6a1784472": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79139a1f358b45b9abc4bfccf9a83ad1",
       "IPY_MODEL_1ce4c656a6fc41e0b43ea752d4763d82",
       "IPY_MODEL_f4cb71ac5bc749a8ae6ff68264418fc8"
      ],
      "layout": "IPY_MODEL_2d22f91d9a5c43e5ae03f3f3da633537"
     }
    },
    "dbb7e4188c644c648802840c1aff9047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc8c77da03654564a0643eff12af1514": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1c271234ea143dc8a0305965c7619b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_850e217b2a9a4e07932c19d002af648c",
      "max": 359199902,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b74b456daca640148f2a5a69b0eb09ff",
      "value": 359199902
     }
    },
    "e28d267a1bf446dbb76e54a9b173fda1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e895dfa521254c4893abd2feda378265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99651468e75143b3b4262235bcb3b66b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7c2591b7c1c43639e61d831f146640f",
      "value": " 1.11k/1.11k [00:00&lt;00:00, 9.61kB/s]"
     }
    },
    "f43c369c08ab49cdb533997e20de6be5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4cb71ac5bc749a8ae6ff68264418fc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc8c77da03654564a0643eff12af1514",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_19c79ce6c38b49f590f781f0de30ebaa",
      "value": " 40.0/40.0 [00:00&lt;00:00, 1.34kB/s]"
     }
    },
    "f8d5d0aa2dc647efb876a83dc530a16a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fce89f9021484783b47b7ebe446efb22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e28d267a1bf446dbb76e54a9b173fda1",
      "max": 1110,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b8b76702a3147ea871e397605a21d93",
      "value": 1110
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
