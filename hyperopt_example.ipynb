{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296b7829-0ad5-460c-adb9-c1ae467230c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCES\n",
    "# https://srijithr.gitlab.io/post/comet/\n",
    "# https://huggingface.co/blog/ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208f0a3c-2ac7-41f8-96a4-7c61080cb15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Comet API key is valid\n",
      "COMET INFO: Comet API key saved in /home/jovyan/.comet.config\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "api_key = \"YOUR API KEY\"\n",
    "project_name=\"CDNA_BERT\"\n",
    "comet_ml.init(project_name=project_name, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c09f04b-f931-4958-85d5-22cbf958bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForSequenceClassification, DataCollatorForLanguageModeling, TextDataset\n",
    "from transformers import DistilBertConfig, DistilBertForMaskedLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"armheb/DNA_bert_6\")\n",
    "model_config = DistilBertConfig(vocab_size=len(tokenizer.vocab), max_position_embeddings=512, num_hidden_layers=3)\n",
    "model = DistilBertForMaskedLM(config=model_config)\n",
    "model.init_weights()\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d88aba-80a0-4d48-87a8-ad7285f0640e",
   "metadata": {},
   "source": [
    "# COMET HYPERPARAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a553c4f8-13b0-4acf-930f-97e8349c8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to optimize\n",
    "def run_train(LEARNING_RATE, EPOCHS, WEIGHT_DECAY, callbacks):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./model',\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        learning_rate=LEARNING_RATE, #5e-5, #2e-5 3\n",
    "        weight_decay=WEIGHT_DECAY, \n",
    "        push_to_hub=False,\n",
    "        per_device_train_batch_size=64,\n",
    "        # gradient_accumulation_steps=1,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=1000,       \n",
    "        save_steps=5000,\n",
    "        fp16=True,\n",
    "        # warmup_steps=1000,\n",
    "    )\n",
    "    \n",
    "    #getting only 2% of datasets for faster hyperopt demonstration\n",
    "    train_dset = load_dataset(\"simecek/Human_DNA_v0_DNABert6tokenized\", split='train[:2%]')\n",
    "    test_dset = load_dataset(\"simecek/Human_DNA_v0_DNABert6tokenized\", split='test[:2%]')\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dset,\n",
    "        eval_dataset=test_dset,\n",
    "        callbacks=callbacks,\n",
    "        )\n",
    "    train_loss = trainer.train().training_loss\n",
    "    eval_loss = trainer.evaluate()['eval_loss']\n",
    "    return train_loss, eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6ebdf2-2268-49b4-a9d8-fb767ddd6c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Passing Experiment through Optimizer constructor is deprecated; pass them to Optimizer.get_experiments or Optimizer.next\n",
      "COMET INFO: COMET_OPTIMIZER_ID=0b9aa2a6df384f95ab93e457ee70a6d8\n",
      "COMET INFO: Using optimizer config: {'algorithm': 'bayes', 'configSpaceSize': 'infinite', 'endTime': None, 'id': '0b9aa2a6df384f95ab93e457ee70a6d8', 'lastUpdateTime': None, 'maxCombo': 0, 'name': '0b9aa2a6df384f95ab93e457ee70a6d8', 'parameters': {'EPOCHS': {'type': 'discrete', 'values': [1, 2, 3]}, 'LEARNING_RATE': {'type': 'discrete', 'values': [5e-05, 0.0005, 0.005, 0.05]}, 'WEIGHT_DECAY': {'max': 0.5, 'min': 0.0, 'scalingType': 'uniform', 'type': 'float'}}, 'predictor': None, 'spec': {'gridSize': 10, 'maxCombo': 0, 'metric': 'valid_loss', 'minSampleSize': 100, 'objective': 'minimize', 'retryAssignLimit': 0, 'retryLimit': 1000, 'seed': 1}, 'startTime': 44497338185, 'state': {'mode': None, 'seed': None, 'sequence': [], 'sequence_i': 0, 'sequence_pid': None, 'sequence_retry': 0, 'sequence_retry_count': 0}, 'status': 'running', 'suggestion_count': 0, 'trials': 1, 'version': '2.0.1'}\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Optimizer, ExistingExperiment\n",
    "#DOCUMENTAITON how to setup: https://www.comet.ml/docs/python-sdk/introduction-optimizer/\n",
    "\n",
    "config = {\n",
    "    # We pick the Bayes algorithm:\n",
    "    \"algorithm\": \"bayes\",\n",
    "\n",
    "    # Declare your hyperparameters in the Vizier-inspired format:\n",
    "    \"parameters\": {\n",
    "        \"LEARNING_RATE\":{\"type\":\"discrete\", \"values\":[5e-5, 5e-4, 5e-3, 5e-2]},\n",
    "        \"WEIGHT_DECAY\":{\"type\":\"float\", \"min\":0.0, \"max\":0.5, \"scalingType\":\"uniform\"},\n",
    "        \"EPOCHS\":{\"type\":\"discrete\", \"values\":[1,2,3]},\n",
    "    },\n",
    "\n",
    "    # Declare what we will be optimizing, and how:\n",
    "    \"spec\": {\n",
    "      \"metric\": \"valid_loss\",\n",
    "      \"objective\": \"minimize\",\n",
    "      \"seed\": 1\n",
    "    },\n",
    "}\n",
    "opt = Optimizer(config, project_name=project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c60fa105-77d4-4745-8f99-8a53ef955ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import importlib.util\n",
    "import json\n",
    "import numbers\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_has_comet = importlib.util.find_spec(\"comet_ml\") is not None and os.getenv(\"COMET_MODE\", \"\").upper() != \"DISABLED\"\n",
    "\n",
    "class CustomCometCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A [`TrainerCallback`] that sends the logs to [Comet ML](https://www.comet.ml/site/).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, experiment):\n",
    "        if not _has_comet:\n",
    "            raise RuntimeError(\"CometCallback requires comet-ml to be installed. Run `pip install comet-ml`.\")\n",
    "        self._initialized = False\n",
    "        self._log_assets = False\n",
    "        self.experiment=experiment\n",
    "\n",
    "    def setup(self, args, state, model):\n",
    "        \"\"\"\n",
    "        Setup the optional Comet.ml integration.\n",
    "        Environment:\n",
    "            COMET_MODE (`str`, *optional*):\n",
    "                Whether to create an online, offline experiment or disable Comet logging. Can be \"OFFLINE\", \"ONLINE\",\n",
    "                or \"DISABLED\". Defaults to \"ONLINE\".\n",
    "            COMET_PROJECT_NAME (`str`, *optional*):\n",
    "                Comet project name for experiments\n",
    "            COMET_OFFLINE_DIRECTORY (`str`, *optional*):\n",
    "                Folder to use for saving offline experiments when `COMET_MODE` is \"OFFLINE\"\n",
    "            COMET_LOG_ASSETS (`str`, *optional*):\n",
    "                Whether or not to log training assets (tf event logs, checkpoints, etc), to Comet. Can be \"TRUE\", or\n",
    "                \"FALSE\". Defaults to \"TRUE\".\n",
    "        For a number of configurable items in the environment, see\n",
    "        [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).\n",
    "        \"\"\"\n",
    "        self._initialized = True\n",
    "        log_assets = os.getenv(\"COMET_LOG_ASSETS\", \"FALSE\").upper()\n",
    "        if log_assets in {\"TRUE\", \"1\"}:\n",
    "            self._log_assets = True\n",
    "        if state.is_world_process_zero:\n",
    "            comet_mode = os.getenv(\"COMET_MODE\", \"ONLINE\").upper()\n",
    "            experiment = None\n",
    "            experiment_kwargs = {\"project_name\": os.getenv(\"COMET_PROJECT_NAME\", \"huggingface\")}\n",
    "            if comet_mode == \"ONLINE\":\n",
    "                # experiment = comet_ml.Experiment(**experiment_kwargs)\n",
    "                experiment = self.experiment\n",
    "                experiment.log_other(\"Created from\", \"transformers\")\n",
    "                logger.info(\"Automatic Comet.ml online logging enabled\")\n",
    "            elif comet_mode == \"OFFLINE\":\n",
    "                experiment_kwargs[\"offline_directory\"] = os.getenv(\"COMET_OFFLINE_DIRECTORY\", \"./\")\n",
    "                experiment = comet_ml.OfflineExperiment(**experiment_kwargs)\n",
    "                experiment.log_other(\"Created from\", \"transformers\")\n",
    "                logger.info(\"Automatic Comet.ml offline logging enabled; use `comet upload` when finished\")\n",
    "            if experiment is not None:\n",
    "                experiment._set_model_graph(model, framework=\"transformers\")\n",
    "                experiment._log_parameters(args, prefix=\"args/\", framework=\"transformers\")\n",
    "                if hasattr(model, \"config\"):\n",
    "                    experiment._log_parameters(model.config, prefix=\"config/\", framework=\"transformers\")\n",
    "\n",
    "    def on_train_begin(self, args, state, control, model=None, **kwargs):\n",
    "        if not self._initialized:\n",
    "            self.setup(args, state, model)\n",
    "\n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        if not self._initialized:\n",
    "            self.setup(args, state, model)\n",
    "        if state.is_world_process_zero:\n",
    "            experiment = comet_ml.config.get_global_experiment()\n",
    "            if experiment is not None:\n",
    "                experiment._log_metrics(logs, step=state.global_step, epoch=state.epoch, framework=\"transformers\")\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self._initialized and state.is_world_process_zero:\n",
    "            experiment = comet_ml.config.get_global_experiment()\n",
    "            if experiment is not None:\n",
    "                if self._log_assets is True:\n",
    "                    logger.info(\"Logging checkpoints. This may take time.\")\n",
    "                    experiment.log_asset_folder(\n",
    "                        args.output_dir, recursive=True, log_file_name=True, step=state.global_step\n",
    "                    )\n",
    "                # experiment.end()\n",
    "                \n",
    "                \n",
    "# cb = CustomCometCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e943ce4a-c2e5-4f4c-974c-10b84e67ce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/vlasta/cdna-bert/3e31881da9c34f68b4dc07b4810c70f6\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from : transformers\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     args/_n_gpu                             : 1\n",
      "COMET INFO:     args/_no_sync_in_gradient_accumulation  : True\n",
      "COMET INFO:     args/_setup_devices                     : cuda:0\n",
      "COMET INFO:     args/adafactor                          : False\n",
      "COMET INFO:     args/adam_beta1                         : 0.9\n",
      "COMET INFO:     args/adam_beta2                         : 0.999\n",
      "COMET INFO:     args/adam_epsilon                       : 1e-08\n",
      "COMET INFO:     args/auto_find_batch_size               : False\n",
      "COMET INFO:     args/bf16                               : False\n",
      "COMET INFO:     args/bf16_full_eval                     : False\n",
      "COMET INFO:     args/data_seed                          : None\n",
      "COMET INFO:     args/dataloader_drop_last               : False\n",
      "COMET INFO:     args/dataloader_num_workers             : 0\n",
      "COMET INFO:     args/dataloader_pin_memory              : True\n",
      "COMET INFO:     args/ddp_bucket_cap_mb                  : None\n",
      "COMET INFO:     args/ddp_find_unused_parameters         : None\n",
      "COMET INFO:     args/debug                              : []\n",
      "COMET INFO:     args/deepspeed                          : None\n",
      "COMET INFO:     args/device                             : cuda:0\n",
      "COMET INFO:     args/disable_tqdm                       : False\n",
      "COMET INFO:     args/do_eval                            : True\n",
      "COMET INFO:     args/do_predict                         : False\n",
      "COMET INFO:     args/do_train                           : False\n",
      "COMET INFO:     args/eval_accumulation_steps            : None\n",
      "COMET INFO:     args/eval_batch_size                    : 8\n",
      "COMET INFO:     args/eval_delay                         : 0\n",
      "COMET INFO:     args/eval_steps                         : 1000\n",
      "COMET INFO:     args/evaluation_strategy                : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/fp16                               : True\n",
      "COMET INFO:     args/fp16_backend                       : auto\n",
      "COMET INFO:     args/fp16_full_eval                     : False\n",
      "COMET INFO:     args/fp16_opt_level                     : O1\n",
      "COMET INFO:     args/fsdp                               : []\n",
      "COMET INFO:     args/fsdp_min_num_params                : 0\n",
      "COMET INFO:     args/full_determinism                   : False\n",
      "COMET INFO:     args/gradient_accumulation_steps        : 1\n",
      "COMET INFO:     args/gradient_checkpointing             : False\n",
      "COMET INFO:     args/greater_is_better                  : False\n",
      "COMET INFO:     args/group_by_length                    : False\n",
      "COMET INFO:     args/half_precision_backend             : amp\n",
      "COMET INFO:     args/hub_model_id                       : None\n",
      "COMET INFO:     args/hub_private_repo                   : False\n",
      "COMET INFO:     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "COMET INFO:     args/hub_token                          : None\n",
      "COMET INFO:     args/ignore_data_skip                   : False\n",
      "COMET INFO:     args/include_inputs_for_metrics         : False\n",
      "COMET INFO:     args/label_names                        : None\n",
      "COMET INFO:     args/label_smoothing_factor             : 0.0\n",
      "COMET INFO:     args/learning_rate                      : 0.0005\n",
      "COMET INFO:     args/length_column_name                 : length\n",
      "COMET INFO:     args/load_best_model_at_end             : True\n",
      "COMET INFO:     args/local_process_index                : 0\n",
      "COMET INFO:     args/local_rank                         : -1\n",
      "COMET INFO:     args/log_level                          : -1\n",
      "COMET INFO:     args/log_level_replica                  : -1\n",
      "COMET INFO:     args/log_on_each_node                   : True\n",
      "COMET INFO:     args/logging_dir                        : ./model/runs/Jun20_20-03-35_jupyter-martinekv---47-50-55-5f-4d-45-54-41-48-4f-4d-45-5f-53\n",
      "COMET INFO:     args/logging_first_step                 : False\n",
      "COMET INFO:     args/logging_nan_inf_filter             : True\n",
      "COMET INFO:     args/logging_steps                      : 1000\n",
      "COMET INFO:     args/logging_strategy                   : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "COMET INFO:     args/max_grad_norm                      : 1.0\n",
      "COMET INFO:     args/max_steps                          : -1\n",
      "COMET INFO:     args/metric_for_best_model              : loss\n",
      "COMET INFO:     args/mp_parameters                      : \n",
      "COMET INFO:     args/n_gpu                              : 1\n",
      "COMET INFO:     args/no_cuda                            : False\n",
      "COMET INFO:     args/num_train_epochs                   : 2\n",
      "COMET INFO:     args/optim                              : OptimizerNames.ADAMW_HF\n",
      "COMET INFO:     args/output_dir                         : ./model\n",
      "COMET INFO:     args/overwrite_output_dir               : True\n",
      "COMET INFO:     args/parallel_mode                      : ParallelMode.NOT_PARALLEL\n",
      "COMET INFO:     args/past_index                         : -1\n",
      "COMET INFO:     args/per_device_eval_batch_size         : 8\n",
      "COMET INFO:     args/per_device_train_batch_size        : 64\n",
      "COMET INFO:     args/per_gpu_eval_batch_size            : None\n",
      "COMET INFO:     args/per_gpu_train_batch_size           : None\n",
      "COMET INFO:     args/place_model_on_device              : True\n",
      "COMET INFO:     args/prediction_loss_only               : False\n",
      "COMET INFO:     args/process_index                      : 0\n",
      "COMET INFO:     args/push_to_hub                        : False\n",
      "COMET INFO:     args/push_to_hub_model_id               : None\n",
      "COMET INFO:     args/push_to_hub_organization           : None\n",
      "COMET INFO:     args/push_to_hub_token                  : None\n",
      "COMET INFO:     args/remove_unused_columns              : True\n",
      "COMET INFO:     args/report_to                          : ['comet_ml']\n",
      "COMET INFO:     args/resume_from_checkpoint             : None\n",
      "COMET INFO:     args/run_name                           : ./model\n",
      "COMET INFO:     args/save_on_each_node                  : False\n",
      "COMET INFO:     args/save_steps                         : 5000\n",
      "COMET INFO:     args/save_strategy                      : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/save_total_limit                   : 1\n",
      "COMET INFO:     args/seed                               : 42\n",
      "COMET INFO:     args/sharded_ddp                        : []\n",
      "COMET INFO:     args/should_log                         : True\n",
      "COMET INFO:     args/should_save                        : True\n",
      "COMET INFO:     args/skip_memory_metrics                : True\n",
      "COMET INFO:     args/tf32                               : None\n",
      "COMET INFO:     args/tpu_metrics_debug                  : False\n",
      "COMET INFO:     args/tpu_num_cores                      : None\n",
      "COMET INFO:     args/train_batch_size                   : 64\n",
      "COMET INFO:     args/use_legacy_prediction_loop         : False\n",
      "COMET INFO:     args/warmup_ratio                       : 0.0\n",
      "COMET INFO:     args/warmup_steps                       : 0\n",
      "COMET INFO:     args/weight_decay                       : 0.24253486187906553\n",
      "COMET INFO:     args/world_size                         : 1\n",
      "COMET INFO:     args/xpu_backend                        : None\n",
      "COMET INFO:     config/_auto_class                      : None\n",
      "COMET INFO:     config/_name_or_path                    : \n",
      "COMET INFO:     config/activation                       : gelu\n",
      "COMET INFO:     config/add_cross_attention              : False\n",
      "COMET INFO:     config/architectures                    : None\n",
      "COMET INFO:     config/attention_dropout                : 0.1\n",
      "COMET INFO:     config/attribute_map                    : {'hidden_size': 'dim', 'num_attention_heads': 'n_heads', 'num_hidden_layers': 'n_layers'}\n",
      "COMET INFO:     config/bad_words_ids                    : None\n",
      "COMET INFO:     config/bos_token_id                     : None\n",
      "COMET INFO:     config/chunk_size_feed_forward          : 0\n",
      "COMET INFO:     config/cross_attention_hidden_size      : None\n",
      "COMET INFO:     config/decoder_start_token_id           : None\n",
      "COMET INFO:     config/dim                              : 768\n",
      "COMET INFO:     config/diversity_penalty                : 0.0\n",
      "COMET INFO:     config/do_sample                        : False\n",
      "COMET INFO:     config/dropout                          : 0.1\n",
      "COMET INFO:     config/early_stopping                   : False\n",
      "COMET INFO:     config/encoder_no_repeat_ngram_size     : 0\n",
      "COMET INFO:     config/eos_token_id                     : None\n",
      "COMET INFO:     config/exponential_decay_length_penalty : None\n",
      "COMET INFO:     config/finetuning_task                  : None\n",
      "COMET INFO:     config/forced_bos_token_id              : None\n",
      "COMET INFO:     config/forced_eos_token_id              : None\n",
      "COMET INFO:     config/hidden_dim                       : 3072\n",
      "COMET INFO:     config/id2label                         : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "COMET INFO:     config/initializer_range                : 0.02\n",
      "COMET INFO:     config/is_composition                   : False\n",
      "COMET INFO:     config/is_decoder                       : False\n",
      "COMET INFO:     config/is_encoder_decoder               : False\n",
      "COMET INFO:     config/label2id                         : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "COMET INFO:     config/length_penalty                   : 1.0\n",
      "COMET INFO:     config/max_length                       : 20\n",
      "COMET INFO:     config/max_position_embeddings          : 512\n",
      "COMET INFO:     config/min_length                       : 0\n",
      "COMET INFO:     config/model_type                       : distilbert\n",
      "COMET INFO:     config/n_heads                          : 12\n",
      "COMET INFO:     config/n_layers                         : 3\n",
      "COMET INFO:     config/name_or_path                     : \n",
      "COMET INFO:     config/no_repeat_ngram_size             : 0\n",
      "COMET INFO:     config/num_beam_groups                  : 1\n",
      "COMET INFO:     config/num_beams                        : 1\n",
      "COMET INFO:     config/num_labels                       : 2\n",
      "COMET INFO:     config/num_return_sequences             : 1\n",
      "COMET INFO:     config/output_attentions                : False\n",
      "COMET INFO:     config/output_hidden_states             : False\n",
      "COMET INFO:     config/output_scores                    : False\n",
      "COMET INFO:     config/pad_token_id                     : 0\n",
      "COMET INFO:     config/prefix                           : None\n",
      "COMET INFO:     config/problem_type                     : None\n",
      "COMET INFO:     config/pruned_heads                     : {}\n",
      "COMET INFO:     config/qa_dropout                       : 0.1\n",
      "COMET INFO:     config/remove_invalid_values            : False\n",
      "COMET INFO:     config/repetition_penalty               : 1.0\n",
      "COMET INFO:     config/return_dict                      : True\n",
      "COMET INFO:     config/return_dict_in_generate          : False\n",
      "COMET INFO:     config/sep_token_id                     : None\n",
      "COMET INFO:     config/seq_classif_dropout              : 0.2\n",
      "COMET INFO:     config/sinusoidal_pos_embds             : False\n",
      "COMET INFO:     config/task_specific_params             : None\n",
      "COMET INFO:     config/temperature                      : 1.0\n",
      "COMET INFO:     config/tie_encoder_decoder              : False\n",
      "COMET INFO:     config/tie_word_embeddings              : True\n",
      "COMET INFO:     config/tokenizer_class                  : None\n",
      "COMET INFO:     config/top_k                            : 50\n",
      "COMET INFO:     config/top_p                            : 1.0\n",
      "COMET INFO:     config/torch_dtype                      : None\n",
      "COMET INFO:     config/torchscript                      : False\n",
      "COMET INFO:     config/transformers_version             : None\n",
      "COMET INFO:     config/typical_p                        : 1.0\n",
      "COMET INFO:     config/use_bfloat16                     : False\n",
      "COMET INFO:     config/use_return_dict                  : True\n",
      "COMET INFO:     config/vocab_size                       : 4101\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info               : 1\n",
      "COMET INFO:     conda-specification      : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (451.99 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/vlasta/cdna-bert/42b73618524244aeb7acb591ff308010\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using custom data configuration simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/simecek___parquet/simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "Using custom data configuration simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/simecek___parquet/simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "Using amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17175\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 269\n",
      "COMET INFO: Optimizer metrics is 'valid_loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/vlasta/cdna-bert/42b73618524244aeb7acb591ff308010\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     optimizer_count        : 5\n",
      "COMET INFO:     optimizer_id           : 0b9aa2a6df384f95ab93e457ee70a6d8\n",
      "COMET INFO:     optimizer_metric       : valid_loss\n",
      "COMET INFO:     optimizer_metric_value : 1\n",
      "COMET INFO:     optimizer_name         : 0b9aa2a6df384f95ab93e457ee70a6d8\n",
      "COMET INFO:     optimizer_objective    : minimum\n",
      "COMET INFO:     optimizer_parameters   : {\"EPOCHS\": 1, \"LEARNING_RATE\": 0.005, \"WEIGHT_DECAY\": 0.07731156854466875}\n",
      "COMET INFO:     optimizer_pid          : 9bc14ff55fa85a1c63317f9ad1c64ec7691bce37\n",
      "COMET INFO:     optimizer_process      : 53244\n",
      "COMET INFO:     optimizer_trial        : 1\n",
      "COMET INFO:     optimizer_version      : 2.0.1\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     EPOCHS        : 1\n",
      "COMET INFO:     LEARNING_RATE : 0.005\n",
      "COMET INFO:     WEIGHT_DECAY  : 0.07731156854466875\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info               : 1\n",
      "COMET INFO:     conda-specification      : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (451.99 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/vlasta/cdna-bert/8d4a78941d09487fbc42f3234c5789a4\n",
      "\n",
      "Automatic Comet.ml online logging enabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='269' max='269' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [269/269 01:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/vlasta/cdna-bert/8d4a78941d09487fbc42f3234c5789a4\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     epoch                    : 1.0\n",
      "COMET INFO:     loss [27]                : (7.985511302947998, 8.048103332519531)\n",
      "COMET INFO:     total_flos               : 1153441439769600.0\n",
      "COMET INFO:     train_loss               : 8.020317219447026\n",
      "COMET INFO:     train_runtime            : 69.7678\n",
      "COMET INFO:     train_samples_per_second : 246.174\n",
      "COMET INFO:     train_steps_per_second   : 3.856\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from : transformers\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     args/_n_gpu                             : 1\n",
      "COMET INFO:     args/_no_sync_in_gradient_accumulation  : True\n",
      "COMET INFO:     args/_setup_devices                     : cuda:0\n",
      "COMET INFO:     args/adafactor                          : False\n",
      "COMET INFO:     args/adam_beta1                         : 0.9\n",
      "COMET INFO:     args/adam_beta2                         : 0.999\n",
      "COMET INFO:     args/adam_epsilon                       : 1e-08\n",
      "COMET INFO:     args/auto_find_batch_size               : False\n",
      "COMET INFO:     args/bf16                               : False\n",
      "COMET INFO:     args/bf16_full_eval                     : False\n",
      "COMET INFO:     args/data_seed                          : None\n",
      "COMET INFO:     args/dataloader_drop_last               : False\n",
      "COMET INFO:     args/dataloader_num_workers             : 0\n",
      "COMET INFO:     args/dataloader_pin_memory              : True\n",
      "COMET INFO:     args/ddp_bucket_cap_mb                  : None\n",
      "COMET INFO:     args/ddp_find_unused_parameters         : None\n",
      "COMET INFO:     args/debug                              : []\n",
      "COMET INFO:     args/deepspeed                          : None\n",
      "COMET INFO:     args/device                             : cuda:0\n",
      "COMET INFO:     args/disable_tqdm                       : False\n",
      "COMET INFO:     args/do_eval                            : True\n",
      "COMET INFO:     args/do_predict                         : False\n",
      "COMET INFO:     args/do_train                           : False\n",
      "COMET INFO:     args/eval_accumulation_steps            : None\n",
      "COMET INFO:     args/eval_batch_size                    : 8\n",
      "COMET INFO:     args/eval_delay                         : 0\n",
      "COMET INFO:     args/eval_steps                         : 1000\n",
      "COMET INFO:     args/evaluation_strategy                : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/fp16                               : True\n",
      "COMET INFO:     args/fp16_backend                       : auto\n",
      "COMET INFO:     args/fp16_full_eval                     : False\n",
      "COMET INFO:     args/fp16_opt_level                     : O1\n",
      "COMET INFO:     args/fsdp                               : []\n",
      "COMET INFO:     args/fsdp_min_num_params                : 0\n",
      "COMET INFO:     args/full_determinism                   : False\n",
      "COMET INFO:     args/gradient_accumulation_steps        : 1\n",
      "COMET INFO:     args/gradient_checkpointing             : False\n",
      "COMET INFO:     args/greater_is_better                  : False\n",
      "COMET INFO:     args/group_by_length                    : False\n",
      "COMET INFO:     args/half_precision_backend             : amp\n",
      "COMET INFO:     args/hub_model_id                       : None\n",
      "COMET INFO:     args/hub_private_repo                   : False\n",
      "COMET INFO:     args/hub_strategy                       : HubStrategy.EVERY_SAVE\n",
      "COMET INFO:     args/hub_token                          : None\n",
      "COMET INFO:     args/ignore_data_skip                   : False\n",
      "COMET INFO:     args/include_inputs_for_metrics         : False\n",
      "COMET INFO:     args/label_names                        : None\n",
      "COMET INFO:     args/label_smoothing_factor             : 0.0\n",
      "COMET INFO:     args/learning_rate                      : 0.005\n",
      "COMET INFO:     args/length_column_name                 : length\n",
      "COMET INFO:     args/load_best_model_at_end             : True\n",
      "COMET INFO:     args/local_process_index                : 0\n",
      "COMET INFO:     args/local_rank                         : -1\n",
      "COMET INFO:     args/log_level                          : -1\n",
      "COMET INFO:     args/log_level_replica                  : -1\n",
      "COMET INFO:     args/log_on_each_node                   : True\n",
      "COMET INFO:     args/logging_dir                        : ./model/runs/Jun20_20-06-17_jupyter-martinekv---47-50-55-5f-4d-45-54-41-48-4f-4d-45-5f-53\n",
      "COMET INFO:     args/logging_first_step                 : False\n",
      "COMET INFO:     args/logging_nan_inf_filter             : True\n",
      "COMET INFO:     args/logging_steps                      : 1000\n",
      "COMET INFO:     args/logging_strategy                   : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/lr_scheduler_type                  : SchedulerType.LINEAR\n",
      "COMET INFO:     args/max_grad_norm                      : 1.0\n",
      "COMET INFO:     args/max_steps                          : -1\n",
      "COMET INFO:     args/metric_for_best_model              : loss\n",
      "COMET INFO:     args/mp_parameters                      : \n",
      "COMET INFO:     args/n_gpu                              : 1\n",
      "COMET INFO:     args/no_cuda                            : False\n",
      "COMET INFO:     args/num_train_epochs                   : 1\n",
      "COMET INFO:     args/optim                              : OptimizerNames.ADAMW_HF\n",
      "COMET INFO:     args/output_dir                         : ./model\n",
      "COMET INFO:     args/overwrite_output_dir               : True\n",
      "COMET INFO:     args/parallel_mode                      : ParallelMode.NOT_PARALLEL\n",
      "COMET INFO:     args/past_index                         : -1\n",
      "COMET INFO:     args/per_device_eval_batch_size         : 8\n",
      "COMET INFO:     args/per_device_train_batch_size        : 64\n",
      "COMET INFO:     args/per_gpu_eval_batch_size            : None\n",
      "COMET INFO:     args/per_gpu_train_batch_size           : None\n",
      "COMET INFO:     args/place_model_on_device              : True\n",
      "COMET INFO:     args/prediction_loss_only               : False\n",
      "COMET INFO:     args/process_index                      : 0\n",
      "COMET INFO:     args/push_to_hub                        : False\n",
      "COMET INFO:     args/push_to_hub_model_id               : None\n",
      "COMET INFO:     args/push_to_hub_organization           : None\n",
      "COMET INFO:     args/push_to_hub_token                  : None\n",
      "COMET INFO:     args/remove_unused_columns              : True\n",
      "COMET INFO:     args/report_to                          : ['comet_ml']\n",
      "COMET INFO:     args/resume_from_checkpoint             : None\n",
      "COMET INFO:     args/run_name                           : ./model\n",
      "COMET INFO:     args/save_on_each_node                  : False\n",
      "COMET INFO:     args/save_steps                         : 5000\n",
      "COMET INFO:     args/save_strategy                      : IntervalStrategy.STEPS\n",
      "COMET INFO:     args/save_total_limit                   : 1\n",
      "COMET INFO:     args/seed                               : 42\n",
      "COMET INFO:     args/sharded_ddp                        : []\n",
      "COMET INFO:     args/should_log                         : True\n",
      "COMET INFO:     args/should_save                        : True\n",
      "COMET INFO:     args/skip_memory_metrics                : True\n",
      "COMET INFO:     args/tf32                               : None\n",
      "COMET INFO:     args/tpu_metrics_debug                  : False\n",
      "COMET INFO:     args/tpu_num_cores                      : None\n",
      "COMET INFO:     args/train_batch_size                   : 64\n",
      "COMET INFO:     args/use_legacy_prediction_loop         : False\n",
      "COMET INFO:     args/warmup_ratio                       : 0.0\n",
      "COMET INFO:     args/warmup_steps                       : 0\n",
      "COMET INFO:     args/weight_decay                       : 0.07731156854466875\n",
      "COMET INFO:     args/world_size                         : 1\n",
      "COMET INFO:     args/xpu_backend                        : None\n",
      "COMET INFO:     config/_auto_class                      : None\n",
      "COMET INFO:     config/_name_or_path                    : \n",
      "COMET INFO:     config/activation                       : gelu\n",
      "COMET INFO:     config/add_cross_attention              : False\n",
      "COMET INFO:     config/architectures                    : None\n",
      "COMET INFO:     config/attention_dropout                : 0.1\n",
      "COMET INFO:     config/attribute_map                    : {'hidden_size': 'dim', 'num_attention_heads': 'n_heads', 'num_hidden_layers': 'n_layers'}\n",
      "COMET INFO:     config/bad_words_ids                    : None\n",
      "COMET INFO:     config/bos_token_id                     : None\n",
      "COMET INFO:     config/chunk_size_feed_forward          : 0\n",
      "COMET INFO:     config/cross_attention_hidden_size      : None\n",
      "COMET INFO:     config/decoder_start_token_id           : None\n",
      "COMET INFO:     config/dim                              : 768\n",
      "COMET INFO:     config/diversity_penalty                : 0.0\n",
      "COMET INFO:     config/do_sample                        : False\n",
      "COMET INFO:     config/dropout                          : 0.1\n",
      "COMET INFO:     config/early_stopping                   : False\n",
      "COMET INFO:     config/encoder_no_repeat_ngram_size     : 0\n",
      "COMET INFO:     config/eos_token_id                     : None\n",
      "COMET INFO:     config/exponential_decay_length_penalty : None\n",
      "COMET INFO:     config/finetuning_task                  : None\n",
      "COMET INFO:     config/forced_bos_token_id              : None\n",
      "COMET INFO:     config/forced_eos_token_id              : None\n",
      "COMET INFO:     config/hidden_dim                       : 3072\n",
      "COMET INFO:     config/id2label                         : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "COMET INFO:     config/initializer_range                : 0.02\n",
      "COMET INFO:     config/is_composition                   : False\n",
      "COMET INFO:     config/is_decoder                       : False\n",
      "COMET INFO:     config/is_encoder_decoder               : False\n",
      "COMET INFO:     config/label2id                         : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "COMET INFO:     config/length_penalty                   : 1.0\n",
      "COMET INFO:     config/max_length                       : 20\n",
      "COMET INFO:     config/max_position_embeddings          : 512\n",
      "COMET INFO:     config/min_length                       : 0\n",
      "COMET INFO:     config/model_type                       : distilbert\n",
      "COMET INFO:     config/n_heads                          : 12\n",
      "COMET INFO:     config/n_layers                         : 3\n",
      "COMET INFO:     config/name_or_path                     : \n",
      "COMET INFO:     config/no_repeat_ngram_size             : 0\n",
      "COMET INFO:     config/num_beam_groups                  : 1\n",
      "COMET INFO:     config/num_beams                        : 1\n",
      "COMET INFO:     config/num_labels                       : 2\n",
      "COMET INFO:     config/num_return_sequences             : 1\n",
      "COMET INFO:     config/output_attentions                : False\n",
      "COMET INFO:     config/output_hidden_states             : False\n",
      "COMET INFO:     config/output_scores                    : False\n",
      "COMET INFO:     config/pad_token_id                     : 0\n",
      "COMET INFO:     config/prefix                           : None\n",
      "COMET INFO:     config/problem_type                     : None\n",
      "COMET INFO:     config/pruned_heads                     : {}\n",
      "COMET INFO:     config/qa_dropout                       : 0.1\n",
      "COMET INFO:     config/remove_invalid_values            : False\n",
      "COMET INFO:     config/repetition_penalty               : 1.0\n",
      "COMET INFO:     config/return_dict                      : True\n",
      "COMET INFO:     config/return_dict_in_generate          : False\n",
      "COMET INFO:     config/sep_token_id                     : None\n",
      "COMET INFO:     config/seq_classif_dropout              : 0.2\n",
      "COMET INFO:     config/sinusoidal_pos_embds             : False\n",
      "COMET INFO:     config/task_specific_params             : None\n",
      "COMET INFO:     config/temperature                      : 1.0\n",
      "COMET INFO:     config/tie_encoder_decoder              : False\n",
      "COMET INFO:     config/tie_word_embeddings              : True\n",
      "COMET INFO:     config/tokenizer_class                  : None\n",
      "COMET INFO:     config/top_k                            : 50\n",
      "COMET INFO:     config/top_p                            : 1.0\n",
      "COMET INFO:     config/torch_dtype                      : None\n",
      "COMET INFO:     config/torchscript                      : False\n",
      "COMET INFO:     config/transformers_version             : None\n",
      "COMET INFO:     config/typical_p                        : 1.0\n",
      "COMET INFO:     config/use_bfloat16                     : False\n",
      "COMET INFO:     config/use_return_dict                  : True\n",
      "COMET INFO:     config/vocab_size                       : 4101\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info               : 1\n",
      "COMET INFO:     conda-specification      : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (451.99 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: The Python SDK has 10800 seconds to finish before aborting...\n",
      "COMET INFO: All files uploaded, waiting for confirmation they have been all received\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1908\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='239' max='239' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [239/239 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/vlasta/cdna-bert/638264e17cf74da4a965ec7b487ae3df\n",
      "\n",
      "using `logging_steps` to initialize `eval_steps` to 1000\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using custom data configuration simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/simecek___parquet/simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "Using custom data configuration simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1\n",
      "Reusing dataset parquet (/home/jovyan/.cache/huggingface/datasets/simecek___parquet/simecek--Human_DNA_v0_DNABert6tokenized-9a684042f2db6cd1/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n",
      "Using amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForMaskedLM.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `DistilBertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/home/jovyan/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17175\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 538\n",
      "COMET INFO: Optimizer metrics is 'valid_loss' but no logged values found. Experiment ignored in sweep.\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/vlasta/cdna-bert/638264e17cf74da4a965ec7b487ae3df\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     optimizer_count        : 6\n",
      "COMET INFO:     optimizer_id           : 0b9aa2a6df384f95ab93e457ee70a6d8\n",
      "COMET INFO:     optimizer_metric       : valid_loss\n",
      "COMET INFO:     optimizer_metric_value : 1\n",
      "COMET INFO:     optimizer_name         : 0b9aa2a6df384f95ab93e457ee70a6d8\n",
      "COMET INFO:     optimizer_objective    : minimum\n",
      "COMET INFO:     optimizer_parameters   : {\"EPOCHS\": 2, \"LEARNING_RATE\": 0.0005, \"WEIGHT_DECAY\": 0.04977968537049299}\n",
      "COMET INFO:     optimizer_pid          : 66c0e84955e29509fe761bec5b39455d6274e33b\n",
      "COMET INFO:     optimizer_process      : 53244\n",
      "COMET INFO:     optimizer_trial        : 1\n",
      "COMET INFO:     optimizer_version      : 2.0.1\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     EPOCHS        : 2\n",
      "COMET INFO:     LEARNING_RATE : 0.0005\n",
      "COMET INFO:     WEIGHT_DECAY  : 0.04977968537049299\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     conda-info               : 1\n",
      "COMET INFO:     conda-specification      : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (451.99 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/vlasta/cdna-bert/08c4ad3980084b89a7841efe65da7ee7\n",
      "\n",
      "Automatic Comet.ml online logging enabled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/538 00:54 < 01:14, 4.16 it/s, Epoch 0.84/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m comet_experiment \u001b[38;5;129;01min\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mget_experiments():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#TODO hf creates their own experiment - its a mess https://github.com/huggingface/transformers/blob/v4.20.0/src/transformers/integrations.py#L664\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     comet_experiment\u001b[38;5;241m.\u001b[39madd_tag(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperopt experiment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     train_loss, eval_loss \u001b[38;5;241m=\u001b[39m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomet_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLEARNING_RATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomet_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEPOCHS\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mWEIGHT_DECAY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomet_experiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWEIGHT_DECAY\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mCustomCometCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomet_experiment\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#Add your hyperparams\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# experiment = ExistingExperiment(api_key=api_key, previous_experiment=experiment.get_key())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# experiment = comet_ml.get_global_experiment()\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     comet_experiment\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, train_loss)\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m(LEARNING_RATE, EPOCHS, WEIGHT_DECAY, callbacks)\u001b[0m\n\u001b[1;32m     24\u001b[0m test_dset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimecek/Human_DNA_v0_DNABert6tokenized\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest[:2\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     28\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     33\u001b[0m     )\n\u001b[0;32m---> 34\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtraining_loss\n\u001b[1;32m     35\u001b[0m eval_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loss, eval_loss\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/trainer.py:1554\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1554\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1557\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1558\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1560\u001b[0m ):\n\u001b[1;32m   1561\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/transformers/trainer.py:2193\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_grad_scaling:\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_apex:\n\u001b[1;32m   2195\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m amp\u001b[38;5;241m.\u001b[39mscale_loss(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;28;01mas\u001b[39;00m scaled_loss:\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/comet_ml/monkey_patching.py:312\u001b[0m, in \u001b[0;36mEntrypoint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m             LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    309\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException calling before callback \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, callback, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    310\u001b[0m             )\n\u001b[0;32m--> 312\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# Call after callbacks once we have the return value\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_run:\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/myCudaCondaEnv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for comet_experiment in opt.get_experiments():\n",
    "    #TODO hf creates their own experiment - its a mess https://github.com/huggingface/transformers/blob/v4.20.0/src/transformers/integrations.py#L664\n",
    "\n",
    "    #TODO logs into another experiment\n",
    "    #TODO trainer do_eval to True\n",
    "    comet_experiment.add_tag('hyperopt experiment')\n",
    "\n",
    "    train_loss, eval_loss = run_train(\n",
    "        LEARNING_RATE=comet_experiment.get_parameter('LEARNING_RATE'),\n",
    "        EPOCHS=comet_experiment.get_parameter('EPOCHS'),\n",
    "        WEIGHT_DECAY=comet_experiment.get_parameter('WEIGHT_DECAY'),\n",
    "        callbacks = [CustomCometCallback(comet_experiment)]\n",
    "        #Add your hyperparams\n",
    "    )\n",
    "    # experiment = ExistingExperiment(api_key=api_key, previous_experiment=experiment.get_key())\n",
    "    # experiment = comet_ml.get_global_experiment()\n",
    "    \n",
    "    comet_experiment.log_metric('train_loss', train_loss)\n",
    "    comet_experiment.log_metric('valid_loss', eval_loss)\n",
    "    comet_experiment.end()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c367b6-6591-4e02-aad4-848cf335bf10",
   "metadata": {},
   "source": [
    "# HF HYPERPARAM TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185eb83-1846-41e9-89d3-1169276fe9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir='./model',\n",
    "        overwrite_output_dir=True,\n",
    "        evaluation_strategy = \"steps\",\n",
    "        save_strategy = \"steps\",\n",
    "        learning_rate=LEARNING_RATE, #5e-5, #2e-5 3\n",
    "        weight_decay=WEIGHT_DECAY, \n",
    "        push_to_hub=False,\n",
    "        per_device_train_batch_size=64,\n",
    "        # gradient_accumulation_steps=1,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=1000,       \n",
    "        save_steps=5000,\n",
    "        fp16=True,\n",
    "        # warmup_steps=1000,\n",
    ")\n",
    "    \n",
    "#getting only 2% of datasets for faster hyperopt demonstration\n",
    "train_dset = load_dataset(\"simecek/Human_DNA_v0_DNABert6tokenized\", split='train[:2%]')\n",
    "test_dset = load_dataset(\"simecek/Human_DNA_v0_DNABert6tokenized\", split='test[:2%]')\n",
    "    \n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dset,\n",
    "        eval_dataset=test_dset,\n",
    ")\n",
    "\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"ray\", \n",
    "    n_trials=10 # number of trials\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myCudaCondaEnv]",
   "language": "python",
   "name": "conda-env-myCudaCondaEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
