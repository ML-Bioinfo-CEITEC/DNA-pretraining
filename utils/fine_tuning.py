# -*- coding: utf-8 -*-
"""Fine_tuning (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WFQpqIZ4aDRpdC7HmcckGFmKifQzj-CS
"""


import os
import comet_ml
import torch 
import datasets 
import numpy as np
from datasets import Dataset, DatasetDict, load_metric
from huggingface_hub import notebook_login
from pathlib import Path
import pandas as pd
from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanNontataPromoters
from genomic_benchmarks.loc2seq import download_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding
from transformers import TrainingArguments, Trainer, EarlyStoppingCallback
from genomic_benchmarks.data_check import list_datasets

# todo change-able tokenizer/-ing

def fine_tune(hug_model_link, model_name, dataset_name, epochs):
    
    print(torch.cuda.get_device_name(0))

    download_dataset(dataset_name, version=0)

    tokenizer = AutoTokenizer.from_pretrained("armheb/DNA_bert_6")

    def kmers(s, k=6):
        return [s[i:i + k] for i in range(0, len(s), k) if i + k <= len(s)]

    tmp_dict = {}

    for dset in ['train', 'test']:
        for c in ['negative', 'positive']:
            for f in Path(f'../.genomic_benchmarks/{dataset_name}/{dset}/{c}/').glob('*.txt'):
                txt = f.read_text()
                tmp_dict[f.stem] = (dset, int(c == "positive"), txt)

    df = pd.DataFrame.from_dict(tmp_dict).T.rename(columns = {0: "dset", 1: "cat", 2: "seq"})

    train_valid_split = df.query("dset == 'train'").shape[0] // 100 * 80
    print(df.query("dset == 'train'").shape[0], train_valid_split)

    train_df = df[df['dset']=='train'].iloc[:train_valid_split,:]
    valid_df = df[df['dset']=='train'].iloc[train_valid_split:,:]
    test_df = df[df['dset']=='test']

    datasets = [train_df, valid_df, test_df]

    print(datasets)

    datasets = [Dataset.from_pandas(x) for x in datasets]

    print(datasets)

    def tok_func(x): return tokenizer(" ".join(kmers(x["seq"])))

    datasets = [x.map(tok_func, batched=False).rename_columns({'cat':'labels'}) for x in datasets]

    dds = DatasetDict({
        'train': datasets[0],
        'validation': datasets[1],
        'test':  datasets[2],
    })

    """## 1) Fine-tuning"""

    model = AutoModelForSequenceClassification.from_pretrained(hug_model_link, num_labels=2)

    training_args = TrainingArguments(
        output_dir='./model',          # output directory to where save model checkpoint
        evaluation_strategy="steps",    # evaluate each `logging_steps` steps
        overwrite_output_dir=True,      
        num_train_epochs=epochs,            # number of training epochs, feel free to tweak
        per_device_train_batch_size=32, # the training batch size, put it as high as your GPU memory fits
        gradient_accumulation_steps=2,  # accumulating the gradients before updating the weights
        per_device_eval_batch_size=32,  # evaluation batch size
        logging_steps=20,             # evaluate, log and save model checkpoints every 1000 step
        save_steps=200,
        fp16=True,
        load_best_model_at_end=True,  # whether to load the best model (in terms of loss) at the end of training
        save_total_limit=3,           # whether you don't have much space so you let only 5 model weights saved in the disk
    # There was an error with some recursion call for push_to_hub == True
        push_to_hub=False,
        # hub_model_id="DNADeberta_fine",
        # hub_strategy="every_save"
    )

    def compute_metrics(eval_preds):
        # metric = load_metric("accuracy", "f1")
        metric = load_metric("glue", "mrpc")
        logits, labels = eval_preds
        predictions = np.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dds['train'],
        eval_dataset=dds['validation'],
        tokenizer=tokenizer, 
        compute_metrics=compute_metrics,
    #     early_stopping_patience - considers evaluation calls (for us, steps at the moment)
        callbacks=[EarlyStoppingCallback(early_stopping_patience = 5, early_stopping_threshold = 0.02)],
    )

    trainer.train()
    model.push_to_hub(model_name + dataset_name)

    metric = load_metric("f1", "accuracy")
    test_f1 = metric.compute(predictions = np.argmax(predictions.predictions, axis=-1), references = dds['test']['labels'])
    print(test_f1)

    metric = load_metric("accuracy", "f1")
    test_acc = metric.compute(predictions = np.argmax(predictions.predictions, axis=-1), references = dds['test']['labels'])
    print(test_acc)

    return test_f1, test_acc